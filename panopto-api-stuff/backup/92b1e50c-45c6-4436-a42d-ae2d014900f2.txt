ID: 92b1e50c-45c6-4436-a42d-ae2d014900f2
Title: 6.2 Strassen's algorithm
Category: COMP40008 - Graphs and Algorithms (Spring 2021-2022)
Lecturer: Iain Phillips
Date: 20/02/2021
So I've mentioned Strauss' algorithm for faster matrix multiplication, faster than the usual definition.
0:02
And so let's see how that works. So the key idea is that we get a faster way to multiply.
0:10
And for any equals two. And then we apply that recursively to do the multiplication.
0:20
So let's start with N equals two. Well, then we know by definition that C the product is just going to be what's shown here.
0:27
And okay, we could do it that way. That will be the usual method.
0:39
And then obviously we have the eight multiplications and the four additions that Strassman gives us a different way of doing it.
0:43
And is more complicated. But the advantages that we get down from eight multiplications to seven both vacations.
0:52
Now notice that the additions and subtractions go up to 18.
1:00
So we're slower on that. But they multiplications are what we are trying to reduce here.
1:05
So it's all rather complicated. I'll leave you to cheque that it actually works.
1:10
Obviously quite clever to come up with this. But you can see the seven multiplications. Each of these Xs takes exactly one multiplication to compute.
1:17
And then we just combine them to get the result.
1:26
And a very significant point is that we don't need the fact that both vacation is commutative.
1:31
Of course it is for arithmetic, but not for matrix matrix multiplication.
1:36
But we don't use it here. So we can just simplify this algebraically to get back to the formula on the previous slide.
1:42
So because we're not using committed devotee, then we can actually recursively apply these formula,
1:52
this formula here on this slide to multiply matrices in general.
1:59
So that's the idea. So supposing that the dimension is now an equals two to the K, some power of two.
2:05
Let's divide our matrices up into four quadrants each, as in the displayed formula here.
2:13
So each of these matrices, a IJA, B.J., CIJ has dimensions and over two by an over two.
2:20
And then we can compute the CIJ is just using the formulas for the little CIJ that we have on the previous slide.
2:28
So C1 one is got by these Xs here like that.
2:38
So we just instead of applying this to numbers or whatever in the Matrix matrices we're applying,
2:44
we're thinking of these as actually being an over two dimensional matrices.
2:52
So then we were recursively compute each multiplication by further subdivision.
2:59
So obviously each of these CIJ is relies on multiplications as well.
3:04
And so we have to do them recursively as well. The additions can be done non recursively because we.
3:12
That's easy. And so you have to keep on dividing until you have to go down K times to get down or came on.
3:19
It's one times to get down to the end equals two. Case where we just compute.
3:27
That's the base case where we compute non recursively. OK.
3:31
So now how long does that actually take? We need to do some analysis here.
3:36
So when we're doing these N by N matrices, we need to add or subtract 18 matrices of dimension and over to by an effort to.
3:42
And we need to recursively perform seven multiplications of an over two by end of a two matrices.
3:54
And so let's write down a recurrence relation for the number of arithmetic operations, a K for inputs of size N equals two to the K.
4:01
Well, if K equals zero, then it's just a one by one matrix.
4:12
So we just get a single operation, that multiplication.
4:17
So just one a K, as we say at the top of the slide here, we've got to do seven multiplications of two to the K minus one dimension.
4:20
So that's seven times a K minus one. Whatever that is.
4:35
So we don't know yet. Plus the 18 additions.
4:39
And to do additions is just an over two squared operations each time.
4:43
So that's our recurrence relation. Well, we already sold a recurrence relation for insertion sort sorry.
4:50
Binary sort by sorry. Binary search by repeated expansion.
4:58
And then summing up. So I won't do that here. But that you can try it for yourself if you like.
5:04
And I'll give you the answer, which is a K is seven times seven to the K minus six times four to the K.
5:12
And once we've got this, of course, I mean, that's okay. A certain amount of guesswork is involved in the repeated expansion.
5:21
But we can then prove by induction on K that this that this is the solution to a K.
5:28
So, again, I would recommend you to plug it in here and prove by induction that it is correct.
5:33
Well, where does the end the two point eight 07 come from?
5:40
So let's look at this formula further. So seven to the K is just seven to the log N.
5:44
And fought of the K. Well, that's just N squared.
5:53
So we've got rid of the K now and we're just going directly on and the input size.
5:58
But then seven to the log in can be rewritten as end to the log seven.
6:03
That's just properties of logs. And so now we're more like what we want.
6:08
And the minus six and squared is going to get small. So it's the end to the log.
6:14
Seven is the important term in terms of the order here.
6:19
And log seven is roughly two point eight 07. So we finally see that strauss' matrix multiplication algorithm is order.
6:23
And to the two point eight 07, you might wonder what to do if the end is not a power of two.
6:33
Well, you have to keep on dividing by two to you Strauss' method.
6:42
So what you can do, I mean, there are various solutions.
6:46
But one way you could do this would be just to add an extra dumi row and column to keep the dimension even.
6:49
And then you allow the subdivision and keep on doing that whenever you can't divide exactly by two.
6:55
So Stralsund album is actually a very useful algorithm. Practise for multiplying together large matrices.
7:02
So I think it's fair to say that the Coppersmith,
7:07
Winograd and successor algorithms that are supposedly lower order and strauss' algorithm, they give sort of very high, constant factors.
7:10
And so they only really of use for matrices, which are much larger than we can actually cope with on today's computers.
7:20
But Strauss' algorithm really is a practically useful algorithm. OK.
7:28
So I'll just finish this video by sit ups, noting that Strauss' algorithm is an example of a divide and conquer algorithm.
7:34
And this is a whole category of algorithms.
7:46
And we're going to see some further examples, merge sort and quicksort, notably.
7:51
So the idea in a divide and conquer algorithm is you divide up the problem into a some problems, some number A of size and over B.
7:56
So when I was smaller than the main, the original size.
8:05
And typically these will be non overlapping some problems in strauss' algorithm here.
8:10
We have a equals seven because we've got seven some problems of multiplication of smaller matrices.
8:17
And B calls two because each of them has the dimension has been halved.
8:24
So there might be some extra work to be done to actually set up the sub problems.
8:29
You might have to do some matrix additions as we do here. Then you can solve each sub problem recursively by applying the algorithm on
8:35
the each some problem and then you take the results from the some problems.
8:43
In this case, the seven multiplications and then you combine them to get the result.
8:48
And again, there may be some work to be done in the combining stage.
8:52
We have to do some matrix additions. Here are non recursive work, obviously.
8:57
So and as I say, we'll see further examples of this with merged thought and quicksort.
9:02
And we'll also see a general formula, the master theorem, which allows us to derive the complexity of divide and conquer algorithms.
9:08