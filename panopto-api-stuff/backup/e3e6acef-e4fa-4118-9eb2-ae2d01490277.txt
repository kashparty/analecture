ID: e3e6acef-e4fa-4118-9eb2-ae2d01490277
Title: 5.3 Linear search
Category: COMP40008 - Graphs and Algorithms (Spring 2021-2022)
Lecturer: Iain Phillips
Date: 14/02/2021
So in this part of the course, I want to think about algorithms for solving particular problems.
0:02
But I want to go beyond that to talk about optimality as well.
0:09
And I say, let's see what that means. So, first of all, let's imagine that we've got some problem we'd like to solve.
0:16
Sorting a list, perhaps finding the shortest path in the graph.
0:23
We've already seen how to do that, multiplying two matrices together.
0:26
So this is just some problem P and then let s be the set of all possible solutions for P all algorithms that could solve our problem for us.
0:31
So if the problem was sorting a list, then we have various sorting algorithms, merge,
0:42
sorting quicksort and plus other ones that perhaps we haven't even thought of yet.
0:47
So one question would be, which are the available elder?
0:53
And this is the best I know. You know, various sorting algorithms.
0:56
Which one should I choose? And there could be various criteria.
1:01
But basically, we will rank algorithms according to how fast they run.
1:05
In other words, their time, complexity. So time complexity is lower.
1:10
That means it's a better algorithm. But here is a harder question.
1:15
Can we improve our existing algorithms or have we already found the best possible algorithm for solving our problem?
1:20
P. Well, that's a theoretical question.
1:28
But it has practical implications. After all, if we can improve our existing algorithms, then there's an incentive to go on looking.
1:33
Spend time trying to improve our algorithms.
1:42
On the other hand, if we have already found the best possible algorithm, there's no need to waste time looking for anything else.
1:45
But notice that we must reason about all possible members of our solution said.
1:53
S not just the ones we know about already. And so we can't just say this is the best amongst the hundred sorting algorithms that we know of.
1:59
We've got to think about sorting algorithms that might be devised in the future.
2:11
So we need a different way of reasoning. And typically this would involve trying to come up with a concept that there's a
2:17
minimum amount of work which every member of s every possible algorithm must do.
2:28
So, for instance, if we were sorting a list, it might be that our sorting algorithm would have to at least inspect every element in the list.
2:35
Otherwise, it couldn't be correct.
2:42
And this minimum amount of work will provide a so-called lower bound on the time complexity of any algorithm for our problem.
2:46
P. So we're in the business of finding these lower bounds.
2:54
So which problems are we going to look at? Well, we're going to look at searching a list for a given element and sorting the list.
3:00
And for two reasons. One is that these are frequently used. We need these all the time.
3:10
And so efficiency is particularly important.
3:18
And the second reason is that the analysis is well worked out and indeed optimal, i.e. best possible algorithms are known to solve these problems.
3:21
So here's a diagram to illustrate. What we're trying to do here.
3:33
So here we have the set. As this, the red region or the region surrounded by the red line.
3:37
And these blue dots represent various algorithms that we know about.
3:44
But there can be others, obviously, that we don't know about as yet.
3:51
Can the green line represents the lower bound? That's the amount of work that we know that any algorithm has to do.
3:55
So imagine that time complexity is is going up the page here.
4:02
So high complexity at the top, low complexity at the bottom.
4:06
And so what we know is that no algorithm in S can go below the green line because we have this lower bound that on the amount of work that is needed.
4:10
So therefore work. We don't know what s is like in in in its entirety.
4:25
But if we find an algorithm A which solves our problem and does not go and is actually at the Green Line, then.
4:31
Okay. There might be other algorithms which are equally good, but there can be no algorithm that solves.
4:41
That's better than a because we know we can't go below the green line.
4:47
So what we have to do is to find good algorithms, low complexity.
4:52
Keep on going downwards, as it were. And B, find a good lower bound.
4:59
That's as high as it can be. And hope that the two will actually meet.
5:04
Exactly. And in that case, we will have proof that A is an optimal algorithm for solving our our problem.
5:08
OK. So let's take our first problem, which is very simple problem of searching a list.
5:18
So L is the list of elements of Type D, just some data type.
5:23
And I will give us random access to this. So it's more like an array.
5:28
Ready. Now, I'll call it a list. And the problem is to search this list, to find an element X of type D.
5:32
And if X is in the list, we should return the index search that L.K. equals X.
5:43
And if X is not in the list, we should return not found. Well, here's a particular algorithm.
5:49
I'll call it linear search. A very obvious algorithm.
5:55
We just inspect the members of the list starting from the left l zero l one all the way up to L and minus one in turn.
5:58
And we stop and return the index if X is found, otherwise return not found.
6:06
So I notice that this will find the leftmost occurrence in the list.
6:13
But in my problem, I did not specify which k if there were multiple K then any of them would do so.
6:17
This linear search algorithm does indeed solve the problem fairly obviously.
6:25
Well, how long does linear search take and how should we count that?
6:31
Well, let's count the number of comparisons. In other words,
6:37
the number of times that we compare X what we're searching for with an element of the list L k and the nice thing about counting that is it is,
6:40
General, because that will work for all algorithms for the problem, because they all have to do comparisons.
6:50
And so if we count that, then it doesn't really matter what programming language or what style we're using,
6:57
whether it's a recursive algorithm or anything or maybe not.
7:03
It doesn't really matter. Well, if we look at some examples, what we see immediately is that the number of comparisons varies.
7:09
So let's take N equals four. For instance, let's say that we're looking for the data type is a natural numbers and we're looking for X equals five.
7:17
So a good case will be where five is the first element in the list.
7:27
And in that case, we can stop after one comparison. And that's pretty obviously the best case.
7:33
On the other hand, supposing the five is not in the list. Well, then we'd have to look linear search.
7:39
We'd run all the way along the list and we'd take four comparisons to find.
7:44
But five is not in the list. And pretty obviously, if we generalise up to input size N,
7:49
then you can see that the the best case will be won and the worst case will be n comparisons.
7:58
In the case that the element is not in the list, there is actually another case, of course, where you would get in comparisons,
8:06
which will be if the thing we're searching for because for the first time in at the end of the list, in the end, the end minus one index position.
8:13
So that's what we can say. And so but typically.
8:24
So how do we make sense of the fact that we got a varying number of comparisons?
8:31
Well, we have two kinds of analysis that we'll be interested in.
8:37
The first is worst case analysis. Let's define W of N to be the largest number of comparisons for any input of size N.
8:40
And in our example for linear search, we get W of an equals n.
8:49
The second kind of analysis we might be interested in is average case analysis.
8:55
Let's say that if N is the average number of comparisons for input size N.
8:59
In other words, ranging over all inputs of size. And what is the average number of comparisons we get?
9:05
Well, average cases a bit more complicated because we need to know or at least make a reasonable assumption about the probability distribution.
9:13
It raises questions like how likely is X to be in the list and how likely is it that X is in a certain position in the list.
9:22
So we might be able to make reasonable assumptions about that.
9:31
Or we might know because we're in some problem domain where we happen to know what the answer is or at least have a reasonable estimate of it.
9:36
So which of those is the better analysis? It might seem that average case would be superior.
9:46
It's a more sophisticated analysis. But the worst case analysis has two advantages.
9:53
Firstly, it gives us an actual cast iron guarantee that on input size n that our algorithm
9:59
will never take longer than W of N and average case can't give you that guarantee,
10:05
obviously, because there will be some bad cases that are going to take longer. So that may be what you want to tell your boss,
10:11
that you can guarantee that this algorithm is not going to take longer than this amount of time on that size of input.
10:17
The second advantages that W of any is easier to compute than AFN, as we've already seen in our small example.
10:26
And the good news is that it often gives us similar results. In other words, the same order of complexity.
10:34
So the constant factor might be slightly better for the average case, but we're not getting significantly different results.
10:39
And we took less effort to compute the worst case. You might wonder about best case analysis.
10:47
But that's never a good idea, because no matter how bad your algorithm and how slow it is,
10:55
you can always tune it up to make it look fast on particular inputs.
11:01
Okay, well, now for our lower bound argument.
11:07
Linear search can be varied pretty obviously there are other searching algorithms.
11:13
We could start at the right and end of the list or maybe start in the middle and work outwards or do all the even indexes and then all the old ones.
11:17
So there's different orders we could do it in.
11:28
But intuitively, it can't be improved because we would have to look at everything in the list.
11:31
So what we want to argue, therefore, is that linear search is optimal in the worst case with the end comparisons.
11:38
You can't do better. So let's actually prove that and try and sort of pin down our intuition in a precise way.
11:45
So let's take any algorithm A which so solves the search problem.
11:55
And notice here, I'm talking about any algorithm, including ones that we haven't thought of.
12:00
All we know about this algorithm is that it solves the search problem.
12:05
And what we claim is that if this algorithm returns not found, then it must have inspected every entry off the list.
12:10
Well, let's prove that. Suppose for a contradiction that our algorithm did not inspect L.K.
12:19
Well, here is the list that it, eh, was applied to.
12:28
And there is L.K. And of course, because this algorithm returned, not found, and we've assumed that it's correct algorithm, then X is not in the list.
12:33
Well, let's modify L to L prime by putting X into the list in the L.K. position.
12:44
Well, what will you do when applied to L Prime? Well, we know when it was applied to L.
12:51
It gave us not found and we know that it did not inspect L.K.
12:56
So we can fool Algorithm A. Now if even though X is in our prime.
13:02
Since it doesn't inspect that position, then it's going to return.
13:07
Not found. Which is wrong because X is in the list.
13:12
So this is a contradiction.
13:16
And therefore, in the case that any algorithm which is correct return is not found, then it must indeed have inspected every entry of L.
13:18
Which means that it must have done any comparisons in the worst case.
13:29