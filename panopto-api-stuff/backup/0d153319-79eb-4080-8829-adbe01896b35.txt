ID: 0d153319-79eb-4080-8829-adbe01896b35
Title: L03 - Floating point numbers (part 1)
Category: COMP40001 - Introduction to Computer Systems (Autumn 2021-2022)
Lecturer: Lluis Vilanova
Date: 12/10/2021
Hello and welcome to our third lecture in introduction to compare systems.
0:08
And in this case, we're going to be talking about floating point number representation.
0:11
So to start to give it a bit of context, why do we need a floating point numbers,
0:18
remember that we have a fixed number of digits in the numbers that we are representing in binary.
0:24
And for example, if we look at the world population right now, there's more than seven billion people.
0:31
And if we look at other measures like light years, that's even many more digits that we need to include or in solar masses.
0:37
There's, as you can see, many, many zeros that we have to encode into the number.
0:49
But at the same time, we can also look at really, really small numbers like the diameter of an electron or the mass of a of an electron,
0:54
two in which we are going to have many decimal digits.
1:04
So then we can also look, for example, at the rational numbers where we need, again, many numbers, and in the case of PI, we need infinitely many.
1:13
And another example of really, really large numbers would be Google, which is on a one followed by 100 zeros.
1:25
Funny story here is that this name was given in the early nineteen hundreds by a US mathematician.
1:37
Story goes that the mathematician realised that that number had no name assigned to it and as his child to give a number to that number.
1:46
And the child simply simply said Google.
1:58
And the story goes that well, many years later, when Google wanted to give a sense of how large was the data they were dealing with,
2:02
they wanted to use the name but misspelled it.
2:14
Whether that is true or not, I guess we will never know.
2:19
But the point here is that we need numbers that can represent both really large values and also really,
2:24
really small values, and we cannot do that with an integer representations that we have seen so far.
2:33
So, for example,
2:42
let's let's look at the example of what would happen with what we know so far if we want to represent an integer with up to 30 decimal digits.
2:44
We would have that we want to do the power of X equals to tend to the power of 30,
2:56
so then we would see that X is the logarithm in base two of 10 to the power of thirty, which gives us roughly 100 beats.
3:03
Or the rule of thumb is that every decimal digit corresponds to around three point thirty two beats,
3:12
which is way more than the eight, 16, 32 or 64 beats that we would have right now in computers.
3:19
So what we have seen so far is evidently not sufficient.
3:27
And if we look at other binary representations, if we look at that same number, we need one hundred and twenty beats.
3:31
And in asking if we are using one byte for you to ask a character that would result in two hundred and four digits.
3:39
I mean, of course, if you remember what I said earlier is the ASCII standard itself only defines seven bits,
3:48
but that would still be more than we can include with what we've seen as a 64 bit architectures.
3:59
So what we can use here is the scientific notation and in decimal unbolting binary,
4:09
the the take the form of and times then in decimal to the power of where and would be the coefficient,
4:18
also known as usually as Malteser and E is known as the exponent.
4:29
And then basically these ten or two, depending on whether we are using a decimal or binary base,
4:36
is exactly that the base which is also known as the ratings.
4:42
So when we look at these two parameters to encode numbers in the scientific notation, we have an.
4:47
There are a number of bids on the exponent that will determine the range,
4:55
how big or small a number we can encode and a number of bids on the acquisition and is going
5:01
to determine the precision or exactness of these a number that we are trying to represent.
5:09
So once we know notice, let's look at how real mathematical numbers compare to this floating point, no representation.
5:17
So if we look at the range in real life numbers, we can encode or express any number between minus infinity to plus infinite.
5:27
And obviously we a floating point given the scientific notation.
5:39
And a fixed number of bids, we will have a finite range, and then if we look at the number of failures,
5:44
even if we have finally drain's still in real numbers, we will have infinite numbers in between any pair of numbers.
5:52
Whereas with a floating point, since we have a limit on the number of digits, we are going to have a finite number of values that we can represent.
6:02
And of course, we will also have problems in what we can call spacing, which doesn't exist in real numbers in maths.
6:14
But in floating point, we are going to see gaps between numbers and these gaps are going up by between each number.
6:24
And we will also see errors in the operations that we perform, which would not happen in one.
6:32
We are dealing purely in maths and in plotting by numbers. The results will be incorrect in some cases, as we will see.
6:40
So one question before we go forward is let's assume that we have signed three digit coefficients and assigned to digit exponent.
6:48
And then the question would be what are the closest floating point numbers that we can express
7:00
that are closest to zero point zero zero one times ten to the power of minus ninety nine?
7:07
And what would be the gap between this number and these two closest numbers around it?
7:15
So to answer this, we can look at these figures, which are the zones of experts ability of a floating point number.
7:23
So, again, we are going to assume.
7:32
Numbers with a same three digit acquisition and assigned to digit exponent and the zone of stability, we will see that here.
7:35
In this range over here, we can go from plus zero point zero zero one times ten to the power of minus ninety
7:46
nine two plus point nine nine nine times 10 to the power of plus ninety nine.
7:55
Anything else that goes above it is going to be an overflow.
8:03
And in fact, we are going to have many gaps here.
8:08
So in in the right side, we are going to have what I just said, positive overflow.
8:12
If we look at the other end of the range, we are going to have negative overflows,
8:19
but also we will not be able to get closer to zero than these numbers over here again, because we our responsibility is limited.
8:25
So we will have a gap between this number that is closest to zero and the actual zero.
8:39
And also we will have multiple gaps in between each of the numbers that are inside the range that we can actually express.
8:46
So as you see, this can sometimes express exactly the number that we want to and sometimes it will
8:56
just get as close as we can with a finite number of digits in the scientific notation.
9:04
So before we look into how to encode it in binary code to perform operations,
9:14
let's look at how we need to normalise so that we have a common way of expressing
9:22
numbers that is always going to be the same across all the operations.
9:30
And this is called normalisation. So this basically means that depending on how we interpret the coefficient,
9:35
a floating point or a sort of floating point number can have multiple forms.
9:43
So if we have here zero point zero two or three times 10 to the power of four,
9:49
we can also express this by moving or shifting the coma to the right as an expression that zero point two or three,
9:55
zero times ten to the power of three. Or we could also shift it even more and express it as two point three times ten to the power of two.
10:05
And also, we could do the opposite. We could express it as zero point zero zero two or three times ten to the power of five.
10:16
So remember, as I said, computers are really dumb,
10:24
so we have to really have one single way to perform an operation and that is the reason why we are going to perform normalisation.
10:26
So, again, as I just said, in this specific implementations,
10:35
we always have to first normalise the number before we perform an operation which is always going to assume a normalised representation.
10:41
And the way in which we are going to normalise the coefficients is that they will always go,
10:51
they will always express values that are in the range of one to where are is the base.
10:57
So for decimal numbers, the coefficient is always going to go from one to 10.
11:03
And note here that this is an open range and for binary numbers, the coefficients are going to be expressed as any number between one and two.
11:08
Of course. So, for example, if we have twenty three point two times ten to the power of four,
11:19
the normalise form would of course be two point thirty two times ten to the power of five.
11:29
And then if we have a negative number minus four point zero, one times ten to the power of minus three, that works exactly the same.
11:35
And in this case, it is already normalised. So we don't need to change anything.
11:45
And again, if we have three four three zero zero zero times ten to the power of zero,
11:51
we will have to normalise these three point forty three times 10 to the power of five.
11:57
And once more, even if we have tons of zeros, like in this case,
12:03
the normalised form would be nine point eighty nine times ten to the power of minus eight.
12:08
And now if we look at binary numbers, that is exactly the same,
12:15
we just need to move the decimal point so that it goes so that we have one single digit before this decimal point.
12:19
So in this case, you see that all of the numbers are going to be one point something.
12:31
So what is important to note here is that the normalised coefficient in a base to an in binary
12:38
will always have a single one in front of the decimal separator unless we want to express zero,
12:46
which is built with, of course, by just writing zero.
12:52
So in binary, what we can do is that we are going to always have this one point, whatever comes after, we will just not represent this first one.
12:57
And therefore, we have we are saving one way that we can express for the rest of the confusion and therefore have a wider range of expressivity.
13:08
So now let's look at Fracture's, let's see how they would work in binary, if we have a zero point one, that would be one half of one in binary.
13:19
So, of course, in decimal it would be zero point five. Then in binary, zero point zero one, that would be a half of that half that we had.
13:34
So, of course, that's going to be a force. And again, for this would be an eight zero point one to five.
13:44
And also we can combine all this binary digits.
13:52
So in this case, we would have one half plus one fourth, which would be 075 and then and half plus force plus eight, which gives us this number.
13:56
And we can have zero in between anywhere.
14:09
We just need to do the appropriate additions to get the decimal equivalent representation and so forth.
14:13
So let's look at it in a bit more of a more formal way, so if we have the binary value,
14:25
zero point zero one one zero one, how do we express it now in decimal?
14:34
So we are going to have each of the beads over here, zero point zero one one zero one.
14:41
And this is the value that each of the digits is going to get.
14:48
So to translate a binary fraction into decimal, we will simply do that.
14:52
It's one divided by four because remember, to do the power of minus two is one divided by four.
14:58
Then we will take this other one, which is dividing by eight and finally take these other digits here, which will be dividing by 32.
15:07
So then we add them all together. And this is the decimal value that we have now.
15:17
Another way in which we can look at this very same way of performing the transition between or the
15:24
transformation between binary and decimal numbers is by saying that this one bit has that value one,
15:33
then two, four, eight, 16 and 32.
15:42
So we are going to add together this eight, four and one, because this means the one we have them here and then divide it by two to the power of five.
15:46
Now by five, this is because this is the number of digits that we have here.
15:59
One, two, three, four, five. And it is exactly the same result.
16:05
These are just two ways of doing the same task.
16:09
So now, of course, if we have zero point zero zero zero one one zero zero one one, we would do exactly the same task.
16:13
This would be. The 30 to this would be the 16 this would.
16:23
Is 16, this would be very fruitful and this would be the one, so we just start from the right side and keep adding them up,
16:29
divided in this case by two to the power of nine, because this is how many digits we have here after on the fractional value.
16:38
All right, so now how do we do the opposite, how do we go,
16:49
how do we go from a decimal value in this case, zero point six eight seven five into binary?
16:54
And as you will see, the process is quite simple also.
17:02
And here we will start with multiplying this by two in both the numerator and denominator,
17:09
so we get one point three seven five divided by two, then we can take that one out and divide it by two.
17:21
Then we are left with the remainder here, zero point three seven five divided by two, which again we multiply by two.
17:29
So we have 075 divided by four. But that is not enough to take it out as one divided by power of the base.
17:38
So we further multiply it by two. So this gives us one point five divided by eight.
17:49
Then we can take it out as one divided by eight plus zero five eight, and then we go multiplying it by two until we get here onto one divided by 16.
17:54
So now the answer, of course, is zero point one zero one one.
18:11
Because remember, this is the this would be the first one, then we have a zero because the division before we skipped it, because it was not enough.
18:17
So we don't have a one here then. This is the second bit is the eighth and the last one is the 16th.
18:29
So now what would a decimal value for zero point one be in binary?
18:41
And with something very simple, zero point one equals one point sixty divided by 16,
18:49
then we can take that one divided by 16 out and we are left with zero point six, divided by 16.
18:56
And then we keep multiplying like two until we have a four one divided by power of
19:02
the base fraction that we can take out until we get to extract all of the bits.
19:08
So now let's look at how floating point multiplication goes.
19:18
So let's assume here that we have two numbers and one and two,
19:25
which then we are expressing using the scientific notation and one times ten to the power of one and then times and two times ten to the power of two.
19:29
So what we can do is just shuffle all these factors around so that we have at one time and two and then 10 to the power of each one times,
19:44
ten to the power if you do. And as you know from school, once we have this multiplication of powers with the same base,
19:54
we can just do this then to the power of one plus eight to.
20:03
So the bottom line here is that to the floating point multiplication, we will simply multiply the coefficients and the exponent together.
20:09
So, for example, if we have four to six times 10 to the power of six times five point four times ten to the power of minus three,
20:21
we will take the two coefficients out, two point six times five point four.
20:30
Times 10 to the power of three, which is six minus three, and this gives us a result of fourteen point four times ten to the bottom three.
20:41
But now, of course, remember that we have to normalise these results.
20:50
So the final answer after normalisation for this multiplication would be one point four or four times ten to the power of four.
20:54
So remember, as I said, we have a limited number of digits when we're working with computers in this case,
21:05
the examples are for decimal numbers, but with binaries it is exactly the same.
21:13
So we will often find that the coefficient is too large to store in our digit limits.
21:21
So, for example, if you have two digit coefficient here with two point three times ten to the power of one times,
21:33
two point three times ten to the power of one, the result would be five point twenty nine times ten to the power of two.
21:41
But remember that I said that we have two digit coefficient, so now we have to do something about this extra digit that we have here.
21:49
And we have two solutions. One is simply dropping, which is called truncation or by Austinmer and the other is rounding to the nearest integer,
21:59
which would be five point three in this case, as you know, from school.
22:11
Now, the reason why they are called biased and biased errors is because with truncation,
22:17
we are always going to the lowest number that doesn't have too many digits.
22:23
So the errors are biased to one side and with unbiased with rounding,
22:29
the error is said to be unbiased because it always depends on on the value that we are rounding.
22:36
We can either be rounding up or down depending on the actual value.
22:42
So now let's look at how to do floating point additions.
22:48
So what we will see here is that a floating point additions like in this example.
22:54
Will not be as simple coalition addition unless the exponents are the same, and in this case they are not.
23:01
So first of all, what we need to do is to align them what is called aligning the exponents.
23:11
And this can be expressed with this formula here.
23:20
And let's look at an example of what it actually means to do this alignment alignment.
23:26
So what we are going to do is choose the number with the smaller exponent and then we will shift or move the comma.
23:33
Of this coefficient to the corresponding numbers to a corresponding number of digits to the right.
23:43
So if we have four point five times ten to the power of three and six point seven times ten to the power of two.
23:51
The smaller exponent is for this number over here,
24:00
so we are going to align it by writing instead zero point six to seven times ten to the power of three.
24:06
And then we can finally add two coefficients together after the lightning, which gives us five point seventeen times ten to about a three.
24:12
And of course, we have to either round or truncate the number.
24:22
In this case, the example shows rounding. So the final result is going to be five point two times down to the bottom three.
24:26
OK, I hope this is looked quite simple so far.
24:35
Now, again, remember that I said that our range of possibility is limited, so we will have overflows and underflow also on the exponents,
24:41
what we saw so far is that we need to do some kind of truncating or rounding on the coefficients.
24:54
Now, let's see what happens with the exponents.
25:01
So we will see an explosion overflow whenever the result is to light, and that means that when the result,
25:06
the explosion of the result is larger than the maximum explosion that we can express.
25:15
So, for example, if that makes this morning is 999 because we have two digits for the explosion,
25:22
then 10 to the power of ninety nine times ten to the power of ninety nine, that would be 10 to the power of one hundred and ninety eight.
25:29
And that is basically an overflow because we cannot express this with two digits anymore.
25:38
So what would happen here is that we have to somehow signal this and handle this overflow by either setting this
25:44
value as infinity or raising some kind of exception that the programmer in case of a computer would have to handle.
25:55
And of course, we are also going to see cases where the exponents have an overflow event and these will result
26:04
when the explosion is too small compared to the smallest explosion that we can express.
26:13
So, again, if we look at two digit plus sine exponents, if a minimum explosion then would be minus ninety nine,
26:20
we would see that 10 to the power of minus ninety nine times ten to the power of minus
26:29
ninety nine would result in ten to the power minus one hundred and ninety eight.
26:34
And that of course is an underflow because we cannot express it with only two digits.
26:38
So again, we would handle this underflow by either setting the value to zero or raising an exceptional.
26:43
So now let's see another tricky point of the multiple irregularities, that floating point,
26:55
remember that so far we've seen what happens when we can include all the confusion,
27:04
when we cannot the all the exponent, if we compare it to real mathematical numbers.
27:09
And now we are going to see what happens when we are comparing floating point values.
27:17
Because remember, since we are by truncating or rounding numbers, these comparisons will not always be exactly correct.
27:21
So it seems we can produce inexact results during operations,
27:35
we need to compare floating point values that in a way that accounts for results that are close enough so that two numbers will compare as the same,
27:39
even though we had some errors during the competition during this rounding or truncation.
27:49
So if we know the desired magnitude of the results, we can basically adjust for the closeness of these numbers, which is called the Absolom.
27:58
So, for example, if we are comparing A and B, we are in fact going to check that B minus Epsilon is less than A, underpays less than B plus Epsilon.
28:10
So if we are no more precisely comparing with one,
28:25
this would be would mean that we the comparison we are going to end up with is a one minus zero point zero zero zero zero
28:30
zero five minus a sorry is less than eight and then A is less than one plus zero point zero zero zero zero zero five.
28:39
And that translates with a number below.
28:49
So a more general approach here is to calculate the closeness of to numbers
28:56
based on the relative size of the two numbers that we are comparing because.
29:02
The general idea here is that if we have to work,
29:09
if we are comparing to really small numbers that the Epsilon we want to have needs to be small enough that we can detect some differences,
29:13
whatever that is desired.
29:23
And if we are comparing to really big numbers, we are going to ignore differences that are small enough then we can consider as valid to be ignored,
29:27
given that we have a limited size for their representation.
29:40
And with that, we reached the end of the first part of floating point numbers, and I hope to see you on the next one by.
29:46