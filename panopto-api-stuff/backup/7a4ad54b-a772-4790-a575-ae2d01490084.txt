ID: 7a4ad54b-a772-4790-a575-ae2d01490084
Title: 6.1 Orders
Category: COMP40008 - Graphs and Algorithms (Spring 2021-2022)
Lecturer: Iain Phillips
Date: 20/02/2021
So we've already talked about orders and the BACO notation informally.
0:01
But I want to introduce precise definitions and discuss it further because we'll need it for an hour analysing sorting algorithms like Merge Sort.
0:07
So suppose that we've got some algorithm. In the worst case, performance is W N is eight and squared plus 300 N plus 70 if some quadratic or a Gnomeo
0:18
and as N gets large we can see that eight and squared is the most important term.
0:29
And so ignoring the constant we can say that w ania's of order and squared.
0:35
What about a different algorithm that is solving the same problem.
0:42
That say that the worst case performance is W Prime. Dan is two and Q plus and squared plus four.
0:48
Then playing the N cubed is the most important term. So we can say this is order N cubed.
0:53
So therefore the first algorithm is of lower order. Quadratic is better than cubic.
1:00
And so we should prefer the first algorithm for large n.
1:06
Even though it's possible that for small values of N. The second algorithm might give better results because the constant factors might be smaller.
1:11
So possibly four very small inputs. The second one is better, but we're interested in assume Totti behaviour as N gets large.
1:20
I mean, obviously when any small in any algorithm is probably going to be okay, we're interested in performance of algorithms.
1:29
When our large inputs and then it's these differences in order that are going to be the most important consideration.
1:36
So when you calculate their worst case complexity, it's often desirable to ignore the constant factors.
1:49
Let's take the example of matrix multiplication. So we're going to multiply to end by and matrices together to get a third one.
1:54
And this is the usual formula. All we can think of that is being either a definition or an algorithm form matrix multiplications.
2:02
And it's just remind ourselves of how that goes.
2:11
So we've got. Two square matrices.
2:16
And we multiply them together to get see, sir.
2:23
A times, B or C. And if we want to get the eye row and the JF column over here, then we take the ice row here and we take the JF.
2:27
Call them here and we multiply them together using the formula on the left here.
2:42
Now, pretty clearly for each individual entry and see that involves an multiplications and N minus one additions,
2:48
which is two and minus one arithmetic operations. And so measuring arithmetic operations.
2:56
We get the W N equals N squared because there are in squared entries in C times two N minus one.
3:02
But let's say we might decide that multiplication is a much more expensive than addition.
3:10
So might be a better measure to just count the multiplications.
3:15
And in that case we get W N equals N squared times N or an cubed.
3:19
So which of these is the right answer as to the complexity of our algorithm?
3:26
Well, if we just ignore the constant factors and we look at the orders, we can see that these are both ordering cubed.
3:31
And so we don't really have to decide. The constant factor is not that important.
3:38
And, you know, if we get different hardware or we buy a new computer that's twice as fast,
3:44
then the Konsta factories really don't matter or they're not that meaningful.
3:48
So. What kind of orders might we have?
3:56
Well, the polynomial ones we've mentioned. So we've got constant, which we normally writers one and linear and squared quadratic and cubed cubic.
4:02
We could have exponential orders.
4:15
Of course, we prefer not to have those, but we have seen algorithms like the Bellmon held Karpe algorithm, which are exponential time.
4:17
And we're also interested in logarithms. For instance, for binary search, we we saw that W of N is order log N.
4:26
So let's insert those as well. So we could get a more refined hierarchy than just the polynomial listing at the top.
4:34
So we've got between constant and linear, we've got log in which is exponentially better, faster than N.
4:43
And then between and then N squared we can have N log N which is going to be of interest to us for sorting and et cetera, et cetera.
4:52
Of course there are many more orders than this. But these are some of the more common ones.
5:00
So what? How can we define orders? So let's define the big O notation first.
5:06
So a bit of notation are pluses, just the non-negative real numbers.
5:15
And supposing that we've got functions F and G, which are complexity functions which take input sizes,
5:20
measures natural numbers and return non-negative real numbers.
5:27
And we'll say the F is bigo of G, F is bounded by G is the way to think of this, or F is less than or equal to G in a certain sense.
5:33
If there is some input size M.
5:45
And there is some constant, positive, real, not no negative, real,
5:50
no such that for all inputs and great rehaul to M F of N is bounded by some constant in G of N.
5:56
So with the C here allows us to ignore the constant factors by just saying there's some constant factor such that F is Baloji event.
6:07
So let's have a diagram for that. So if we draw.
6:16
Grofe. So here we've got input size and here and here we've got the complexity, the number of operations on this y axis here.
6:23
So I'm saying that F is bounded by G, then maybe here is F, which goes along like that.
6:35
They're relatively slow growing. And then let's put G in red.
6:43
So here is G and GS faster growing asymptotically as N gets large.
6:48
So eventually F is Baloji.
6:56
But notice that there is this initial region in which may be F is actually slower than G, but we allow for that by saying beyond a certain M.
6:58
So maybe the here is M. And beyond that point, we can see that FEMA is bounded by, gee, I can't really put the constant factor into the diagram,
7:10
but in the diagram is sort of suggestive that we're saying beyond for all but of an initial finite portion, Effie's bounded by G.
7:20
Okay. But notice this as a lesson or equals result. So a relation.
7:29
So we could have that F is bounded by G.
7:34
F is big of G. And G is bigo of F. And in that case we'll say that F is order G.
7:39
And they are not the same functions, not precisely equal, but they have the same rates of growth.
7:46
And and so that's our definition of order. So that's a capital Greek theatre.
7:52
But we can pronounce that as Order G. So let's.
7:57
Just illustrate the definition by proving that our quadratic polynomial here is order and square.
8:09
It so has precisely the rate of growth of N. Which is what we've said informally.
8:17
But I emphasise that we're just doing this to illustrate the definition.
8:21
We don't want to every time we see a quadratic polynomial to have to go through a laborious proof of this.
8:24
We can obviously just see, well, N squared is the fastest growing term and we can ignore the constant fact.
8:30
And so we get this immediately. But how could we prove that?
8:36
Well, clearly, one way round is easy because any grade is just bounded by the quadratic here for every value n.
8:41
So that's easy and constant factories. Just one. What about the other way round?
8:50
Well, it might seem that this one is slower than N squared.
8:55
But we were allowed to have a constant factor c. So all we have to do is to take C bigger than eight.
8:59
So let's say that we guessed that C could be nine. Then is this quadratic.
9:05
Less than nine N squared. Well, maybe not for some small value.
9:12
So let's have a look at that. So let's subtract the quadratic from nine N squared.
9:16
That gives us this expression here. Well, plainly, as N gets large, this is positive.
9:23
And so we're okay. But it might be negative for small values of n like one or something of that sort.
9:28
So all we need to do is to choose our M to be sufficiently large to make this
9:36
right and expression positive and M equals a thousand would clearly be enough,
9:41
probably, obviously less would do as well. And but it doesn't matter.
9:46
All we need is some value of M and some value of C.
9:50
And that's all we need to prove our result.
9:55
So we have shown that eight and squared plus three hundred and class seventy is Bago of N squared, which is.
9:57
And then we have it the other way round. And therefore they are the same order.
10:05
And of course we never say that. A polynomial. Sorry.
10:10
Complexity function is order this. This is too complicated. We'll go for the simpler expression.
10:14
In other words, just order in squared. Well, sometimes the exact order is unknown.
10:18
Let's go back to our matrix multiplication example.
10:26
Well, are definition or are our standard algorithm is order and cubed.
10:32
It really does run precisely in order and cubed in terms of counting the number of errors, medical operations.
10:38
But that doesn't mean that there isn't a faster algorithm.
10:46
So all that tells us about the problem of multiplying matrices together is that that problem is bigger than cubed.
10:49
In other words, we could hope to do better than are definitional algorithm.
10:57
And so what's the story here? It's quite interesting, it says this is a very significant problem.
11:04
Obviously matrix multiplication is used all the time. And so we'd like faster algorithms that we can have them.
11:08
Well, the lower bound is is actually and square head.
11:15
And that's easy to see because you have to at least inspect the order and squared entries in the two matrices that you're trying to multiply together.
11:20
So we know that. And the first breakthrough is Strauss' and 1969.
11:31
And he got the order down from end cubed to end the log seven.
11:38
In other ways. And the two point eight 07, approximately the next big breakthrough was the Coppersmith Winogrand algorithm.
11:44
And that got us down to end to the two point three seven six.
11:54
So that's quite a significant improvement. Since then, improvements have been rather harder to come by.
11:59
I've lists some of the more recent developments here, Stockers Williams, Legault and then hot off the press.
12:06
Aumann and Williams are back with this value here.
12:14
So you can see that the improvements are getting less and less. It's rather like the Olympic 100 metres where they have to sort of refine the
12:18
clocks and get an extra decimal place in order to see an improvement going on.
12:26
But the interesting story here is that many researchers think that actually and squared is the real low bounds.
12:32
So the hope is that this can be significantly improved and we can get all the way down to N Square.
12:40
Of course, that's an open problem. So I'll finish this section by just having a look at how to find the orders of various functions.
12:45
These might be complexity functions for some algorithm. So you might want to do this yourself before going any further in the video.
12:58
But then. I'll just go through it on the whiteboard now.
13:08
So what we want to do is to list these in order. Going as, let's say, from left to right in increasing order.
13:13
OK, so let's do that. So.
13:21
So the first firstborn and cubed over two locks, the order of that.
13:33
Well, that's going to be obviously ordering in queue. So let's put that somewhere in the middle.
13:36
And you, David, to and I'll say that that is order ordering.
13:41
Cute. Case because a taken away the constant factor of a half log in.
13:45
That's going to be off to the left here. That's pretty small.
13:51
I think that's about the smallest one on this list. So let's put this off to the left. Okay.
13:55
Log in. Two to the N.
14:00
Well, that's exponential, so that's bigger than all the polynomial. So that should be somewhere off to the right here.
14:05
I don't see anything bigger than that. So let's put that all off. Way off to the right to the N.
14:11
Next one here. I've tried to make this a little bit confusing, but if you look at it, you can see that end to the fifth is the is the big term here.
14:18
So that one should go to the right of here. So that will be order end to the fifth.
14:27
And we can put down and squared minus. And huge loss.
14:34
And the fifth to a log of N squared.
14:41
So that might look like that's bigger than log N. But if you think about it.
14:47
Buy properties of logs. Then that's just two times log in.
14:52
So log off and squared only differs by it is equal to two log n.
14:56
So they are actually at the same order to to the end minus one.
15:04
Well, it looks like it's less than two to the end. Well, it is, but it's only a difference by constant factor.
15:12
So they should belong at the same level. That.
15:18
And over five. Well, that's just Linea's, and it's pretty obvious that can go in here.
15:24
Six and plus log in. Well, logging in is exponentially smaller than the six.
15:32
And so that is just order in as well.
15:39
That's right. That in. So I'll put six in the last log in.
15:43
Just goes there as well. OK. So it asks, is it basically this is linear growth as well.
15:49
And two to the routine. So that's an interesting one.
15:56
Two to the routine is less than two to the end. So let's put it in here.
16:00
Right. And but they don't differ by a constant factor, if you think about it.
16:05
So this is strictly less than two to the N and two to the root is also strictly greater than it grows faster than any polynomial.
16:11
As you can see, if you just take logs of any polynomial and compare that with the log here, which is routine.
16:21
So this is an interesting function because it's super polynomial grows faster than polynomial any polynomial.
16:29
And it's sub exponential below the exponential function.
16:37