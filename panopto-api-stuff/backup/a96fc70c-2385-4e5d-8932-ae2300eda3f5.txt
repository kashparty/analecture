ID: a96fc70c-2385-4e5d-8932-ae2300eda3f5
Title: Lecture 2: Introduction to Computer Architecture (copy)
Category: COMP40005 - Introduction to Computer Architecture (Spring 2021-2022)
Lecturer: Wayne Luk
Date: 20/01/2022
OK, so how do we find out performance of a computer? Well, before it works.
0:10
OK, so there are different perspectives when we considered performance.
0:39
OK, so we could consider from a purchasing perspective, because if you're going to buy a computer,
0:46
obviously you want to make sure the amount of money you get will give you the highest possible performance.
0:53
OK, but then we could also consider from a design perspective.
1:02
So imagine that of the one someone who would become the next generation computer architect.
1:08
OK, so you need to think about how to design a better processor than the competitors because you will be the only one, right?
1:15
And since they're already very well known processes in the markets right now, it's not so easy for a newcomer to beat them.
1:28
OK, so how? How do you do that? OK, now obviously we still need to consider performance and costs, OK?
1:38
But we also need to consider one more.
1:47
Well, global performance and course.
1:51
We're not enough because, you know, if I want to design the next the best processor, I go into the market, I say, Oh, OK,
1:55
I have a survey of what's available now, and I design something which would be five or 10 times better than what's available.
2:05
What should it be sufficient and why is it not sufficient?
2:15
Can somebody? What about the possibility that is going to be a bit is as good point, yes, but but if it is compatible, might not be sufficient, yes.
2:20
So get your design to market means that competitors already got something that's much better, right?
2:31
Yes. So remember that when you design something, it would take some time.
2:37
OK, well, let's say three, three months, six months if you are very lucky to get a new processor onto the market.
2:42
But in six months time, your competitors, they are not standing still.
2:49
OK, so they will also have improvements. So so what we need to do if you're a designer and if you want to have something very successful,
2:55
not only what you need to be better than your competitors products now, but you need to guess.
3:07
What improvements your competence would have in six months time, whatever your product will get out, OK,
3:16
because your product would be compared obviously with the products of your competitors, but it gets out.
3:24
Not now. Yeah. So if you are much better than most now in six months, they'll be almost that.
3:32
OK? So the tricky thing is to estimate is to figure out what a bomb or intel or whoever is designing processes,
3:38
how to be better than those in six months time.
3:51
Whatever you decide, will get out. OK, so that is the tipping point.
3:55
So how do we do this comparison?
4:00
Well, we need something to calculate as a basis for comparison.
4:04
OK. So there are well-known benchmarks, and we will look at some of those in a minute that allow us to carry out such comparisons.
4:12
So in order to estimate performance, turns out there is an equation for us to do that this very simple equation.
4:25
So if you look at your neighbour, if they are those, they arc and you give them a kick,
4:37
OK, because if you forget about everything either of these costs,
4:43
maybe 10 or 20 years time, if you just remember this equation, it really is worth the effort.
4:47
This is the major equation that will be useful, perhaps for the rest of their lives.
4:53
So what is it? OK, well, it just a way to estimate execution time from some of the fundamental parameters of a processor.
5:01
So this parameter is called CPI now, and very important is the average clock cyclosporine function.
5:15
OK, so it's interesting. I think if a number of clock cycles and this CPI is the effort of the OK,
5:26
so for particular programme, it might be different the number of cycles reconstruction.
5:35
OK, so you could sum all the cycles together and you buy another instruction gives you this average CPI.
5:42
So in other words, a number of cycles for the entire programme is obviously is no instructions for the programme multiply by this average CPI.
5:50
OK, so the execution time of this programme is just a cycle time multiply by the number of cycles.
6:03
OK. And since we know the cycle time is the reciprocal of the clock speed.
6:15
OK. Therefore, it is on time is the reciprocal of the cross.
6:21
Multiply by number of cycles, so we know those cycles. The number of instruction multiply CPI.
6:29
So the execution time of a given Graham is just the insertion call, which is number of instructions multiply by the CPI multiplied by cycle time.
6:35
OK. Or one of the clocks, if. All right, so could not be.
6:49
Much harder than a lot simpler than that. OK.
6:57
And of course, to be able to calculate the average time for any programmes that you could just some all the time together to modify that.
7:00
OK, so. Well, maybe we could look at examples how to actually make use of this equation.
7:09
So let's say we have two machines M1 and M2 implement the same instruction set.
7:18
And there are two classes of instructions say and be OK and CPI for M1 on cost based instruction with the A1 and similarly,
7:23
social CPI for one on Class B would be one and the same for two, which would be 08:22 And you'll find that there is no such calculation.
7:34
You said there are many different parameters. So you just need to be very careful to recognise which is which.
7:48
The calculations themselves are actually pretty straightforward. OK.
7:55
This really figuring out which numbers denoting what and what they'll be calculating.
7:58
OK, now and we also provide a clock speed as well.
8:07
So the calculation is to compare the peak and average performance of any instructions.
8:12
Half of that would be class paths that would be crossed. OK.
8:20
So in order to do a such comparison, the way to do it is to find the ratio of execution time.
8:25
OK? You could also check the dimensions to make sure they're consistent.
8:34
So if we just look at peak performance in order to get a peak performance, we obviously need to find the minimum number of cycles.
8:41
It doesn't mean that the peak performance is something that would happen very often in practise.
8:54
It does mean that you cannot get faster performance than that by definition.
9:00
OK, so to calculate that, then we would need to find a minimum CPI for M1.
9:08
OK. And let's say you only have two classes, then it would simply be the minimum of A1 and people.
9:17
OK. And remember that we have instruction.
9:24
Multiply by CPI. Multiply by cycle time and cycle time is just one over.
9:29
OK. So you could see that this execution time for M1 would be that.
9:36
And similarly, decision time frame two would be pretty much the same expression.
9:42
But instead of A1 B1 and see what it become A to B to C to OK.
9:48
And therefore we compare the execution time of one and two is simply a minimum of able and and multiply one over the denominator.
9:54
A minimum of eight would be two multiplied by two.
10:06
OK. And similarly, if you want to compare the average performance given by any instructions relative to the peak instruction.
10:11
OK, well again, that is music forward is really is just some of A1 and be one divided by two.
10:25
OK. And then you could see the result over there. All right. So most calculations.
10:36
So variations of what is solo over that?
10:43
So you could say these things basic forward once you understand the principle of this execution time equation.
10:46
So the key thing is to make sure that we always compare the social time.
10:57
The implication of the performance of a programme rather than any other than the other way another metric.
11:06
OK. So we can actually then type of programme rather than the number of instructions executed into a second for that particular programme?
11:15
OK. Because it turns out that sometimes you might actually have higher instructions the second, but actually longer execution time?
11:25
Yes. So is that possible? But some problems have more extensive structures.
11:36
The other problem is obvious like take more time to execute well.
11:42
Obviously, different programmes could have different number of instructions, right?
11:50
And if in the same programme might end up using different instructions if you use different compilers.
11:56
OK, so so the relationship between a programme and the instruction for that programme might not be the same.
12:04
It depends on different factors. OK. And actually, in a minute, we're going to look at how different things could affect execution time.
12:16
Your question? Actually, straight, right?
12:25
OK, so so the top line is simply a way to check the execution time equation is consistent in dimension.
12:31
OK, so the process of time is obviously second time, right?
12:42
Per programme. OK. And that is the same as the product instruction per programme multiplied by which is the insertion count.
12:50
Multiply by cycles per instruction, OK, which is the CPI multiplied by seconds.
13:00
The code, which is the cycle time.
13:07
OK, so we could check that in the court system, and we could then look at how would the algorithm language compile it and so on.
13:12
How would they affect each of these treatments? OK,
13:25
so if you look at the algorithmic description of obviously you have complex algorithms for simple algorithms that
13:30
would obviously affect the instructions that they would produce good also affect the cycles for instructions.
13:39
So, for example, it could if you have certain operations, then it might need instruction that would take longer number of cycles than otherwise, OK?
13:49
Similarly, for language and compiler that they all have impact on both instruction count and the cyclosporine function because,
14:03
for example, the compiler, if other how smart the computer driver is.
14:16
OK, so the same operation could be implemented by different kinds of instruction because it might not be unique.
14:21
So if a smart compiler could take instructions that take fewer cycles than you could imagine,
14:29
the average cycle for instruction, what that programme would be less.
14:40
OK, now what is interesting is that it is the instruction set that would have impact on all three.
14:45
And that's one of the reasons why we need to pay attention to the instructions.
14:56
OK, well, let me say instruction so it would determine the social, well, local risk assist architectures,
15:02
but you have complex especially you might have fewer inspections and also maybe have fewer instruction,
15:14
but the instruction themselves would be more complex and therefore the CPI would also be different.
15:19
OK.
15:26
And then when we have a more complex instructions that that might mean that the architecture so for instance, that might have a longer cycle time, OK?
15:28
And therefore it also could have impact on the clock for that particular architecture and then for the
15:43
organisation of a particular architecture and also the technology that would just have impact on the top rate.
15:51
And it should not affect CPI itself.
16:02
So we look at what's happened in the last 40 years, and you could see that we have this amazing improvement in performance.
16:07
So the key thing is to recognise that in the vertical axis, it is not linear.
16:21
It is exponential. OK, so the 10 hundred thousand and so on.
16:29
OK, so initially the instructions that tend to be more complex because the compilers are not very advanced.
16:34
OK. So one would try to build hardware that would implement complex instructions directly.
16:45
And that time you could see the order that is still reasonable improvement.
16:53
We have been able to double performance by F3 by about three and a half years.
16:59
OK. But that is really the the risk eight degree that you could see it.
17:07
That's when a lot of improvement in performance had been achieved.
17:17
OK. And that improvement, instead of doubling every four and a half years, is doubling every one and a half years.
17:22
OK, so you could see the slope is a lot steeper. OK.
17:30
But every good thing could come to an end.
17:34
So by around 2005 and onwards, there is this what is known as the end of the day, not scaling.
17:39
And we can look more into what that means later on.
17:49
And it simply means that you probably may realise that after a while, the scaling is such.
17:54
But the processor power consumption could become permanent and therefore.
18:03
Any increase in clock speed in order to just make the process the faster by simply
18:13
reducing the sample time could actually which could result in high power consumption.
18:20
OK. And therefore, is the power consumption that limited the continue scaling of simply reducing the cycle time?
18:29
Okay. So a lot of this improvement before that is simply due to reducing the cycle time,
18:41
just turn up the clock speed and amazingly, your process would become faster.
18:47
You don't really need to do very much. Just make your process smaller and use a high capacity.
18:52
OK. But as I said around 2005, that earns and therefore we could start need to move to multicore so many core architectures.
18:58
OK. And then there would be you could see it become flutter, flutter.
19:10
And the recent trend is that in order to get even higher performance, we need to support the architecture by permitting specific improvements.
19:17
And that is what I show you in the last lecture, where we have the TFEU, we have the matrix multiplier.
19:31
OK, which will just improve application for deep learning.
19:40
So if you have a database of some other things, the multiplier might not help you.
19:45
OK, well, perhaps you don't care too much about database like this when you want to run.
19:50
To do so when you run deep learning keep is very good because it has special support for this matrix operations.
19:57
So let's have a simple example to illustrate a comparison of two architectures.
20:08
So remember, what we want to do is to minimise execution time, which is the product of these three things.
20:16
OK, so so there are two examples at that particular time.
20:22
There was this company called Sun, which I think is part of Oracle now.
20:30
The current product is based on a complex instruction set architecture called the six thousand OK.
20:36
And then that is starting a new workstation, which they want to base on a new risk processor.
20:46
So here are the comparison of the complex instructions, which is the old version versus the new risk processor at that time.
20:56
So you're looking at something called the risk processor will actually get twenty percent more because as you said before, right?
21:08
But you have simpler reduce instructions and we actually have more instruction.
21:16
So instead of the insertion, come. In something it's often called the risk footloose to SysRq, OK, school time.
21:20
Well, usually because the risk processor.
21:30
Has a similar architecture. It should have a shorter cycle time than the corresponding complex in structure.
21:36
OK. No. Having said that, because in this particular case, the risk processor is actually a new processor,
21:46
whereas the 68000 is something that's already been optimised for quite a few years.
21:57
OK, so it turns out that when the newest processor first come along, it's actually slower than the corresponding complex instruction.
22:02
OK. You fact it's 50 percent lower than the complex instruction.
22:13
OK, well, you might say, Oh, there we have three things the risk process actually lose out on two things.
22:19
OK. Well, to win.
22:26
Well, those are the best processor will win on CPI.
22:30
OK. Because so and so.
22:37
But the 68000 would take an average of five two second cycles per instruction, whereas the risk processor would only take one to 1.2, the 1.7 cycles.
22:39
So the CPI, the risk processor with but because of the significant improvement to overcome the loss of interest in cow.
22:56
And also these Typekit. OK.
23:09
And therefore, it turns out that for many programmes, the risks the new risk processor would be twice as fast as the corresponding six thousand.
23:13
OK, but that's not finished yet, although this faster. Well, there is still the price.
23:26
OK, so well, obviously, if you want to have a new processor, you want to get your money back.
23:33
OK, so that you could give a salary increase to the working capital architects and designers.
23:43
So when the new processor come along, they increase the price by 10 to 20 percent.
23:49
OK. So you that a good deal? And what you said is a good deal.
23:56
Well, how can we objectively say it, it's a good two, twice better performance.
24:05
And 20 for expenses. Right? OK. Yeah, because you're talking about that, the way you compare is you could think of it as a linear scale.
24:12
OK, if we're in increased performance twice.
24:24
OK, normally we would expect the prise could also increase twice because if it is proportional.
24:29
But now, in this case, although we have total performance, it don't increase probably by 20 percent.
24:38
Well, looks like a good deal, right? OK, so.
24:43
So that is one way to say we have a faster processor and it's a much more cost effective than the old one.
24:47
So you want another one, right? OK, so so what are the principles of deciding the risk processor?
24:54
Well, first, that example illustrates the key thing is to try to reduce the cyclosporine function.
25:04
OK, because if we do that, then maybe we lose an interest income and so on.
25:12
Ultimately, we could still get the results, but not, of course, where your competitors are low risk processes.
25:17
OK, then you need to be very careful because everybody would be trying to do similar things.
25:24
So we want a simple and regular format.
25:32
We want a small number of new purpose registers, as we could see in place of lives that only 32 general purpose registers.
25:35
So that would reduce cycle time that could run faster. Similarly, the implementation.
25:45
And also, if we have a simple processor, there might be more space on the chip for other things.
25:52
So what are the other things? Well, remember that there is very expensive to fetch data and instruction from external storage into the chip.
26:03
So how about if we use a lot of the area of the chip sets?
26:15
Very fast memory? OK. So that's why we if you look at the picture of many chips,
26:23
you can see there are actually quite a lot of space being used as both fast memory, which the point, though, is called cache.
26:31
OK. So we put a lot of the cache on the processor itself in order to make sure we could have very fast access to this memory.
26:39
OK. And of course, if you have simply a subset of the easier to use comparable database might also be simple as well,
26:53
because if the instruction set is simpler and of course,
27:01
this is very important to make sure that the implementation is correct,
27:06
because if you make a mistake in the implementation, then you've got some of the instructions wrong.
27:14
The might be sued by some of the users. OK.
27:20
So when one has a simple instructions that it is likely that checking and verifying
27:24
that your implantation is correct is much easier than complex instructions.
27:34
But also, if you have a smaller chip, then it could be cheaper so you can see it.
27:41
There are very good reasons why one may want to have this instruction.
27:46
No say well, but a lot of this processor might not be this with instructions that now why is that?
27:54
Well, there is one thing which is desirable but not mentioned on this list of things.
28:04
Can somebody say that? I think somebody actually said that before?
28:12
So that is when you have a software base and there are already existing programmes in a certain instruction.
28:18
So for example, in the case of Intel, they already have software using the complex instruction of x86.
28:30
OK. And it is that need for compatibility that really provide one of the motivation.
28:38
But many of the processes are still from Intel until the contract is OK.
28:47
But increasingly, they become less competitive because of reasons.
28:54
So we'll be here. OK. So in order to provide a high performance of some of the reasons like I already mentioned,
29:00
have false memory on the chip itself right next to the processor, so we do not need to go off the chip.
29:12
A lot of processors, as you know, these days, they are called multi-core.
29:21
Many call OK, so many positive elements are actually on the chip itself, not just a single one.
29:26
OK. So the modem functional units and they are arranged in a pipeline fashion, low production pipeline.
29:35
So there again, the problem with them when we process the data in this pipeline.
29:43
OK. And then model in an stream could also be processed simultaneously.
29:50
So sometimes this is called multithreading. OK, so we have what is called the super scale of processing.
29:58
We have a pipeline processing with a modified thing and so on.
30:05
So again, a lot of these architectures,
30:09
but they explain in more detail later on off example is the advanced architecture course in the first year and et cetera.
30:12
And then there are also possibilities for introducing some of these processing theoretically in hardware,
30:22
using the programmable hardware devices that I mentioned before.
30:28
OK. Because now that the case in the Connexions can only be programmable, okay.
30:33
Well, what are the monitors? Well, the monitors that we could reprogramme the logic case and the Connexions instead of fetching instructions.
30:40
OK, so let me still over her in fetching and decode and execute instruction if you want to execute a different programme.
30:51
We just reconfigured the chip. OK, so that's another way of supporting the next generation of computer systems.
30:59
And then there are other technologies that so many of you have thought of GPUs,
31:09
and they are examples of a particular kind of multiple processors that takes a single instruction but have multiple threats in those architectures.
31:15
And some of them might say, well, maybe the core is the problem, and they try to find different ways of optimising the clock.
31:28
Maybe you diminishing it as well.
31:35
OK, so so let us now change the discussion a little bit and look at how we actually compare performance using this benchmark,
31:37
which are specific set of programmes designed to compare different architectures
31:49
which can be used to guide different design decisions or different architectures.
31:54
But since we actually look at the result, we could also compare compilers as well, OK, or even compare algorithms.
32:01
So you could look at multiple things, but if you do that, we need to make sure that we need to keep some of these things constant, right?
32:12
Because you can be aggressive, but you use different compilers, but then you have multiple factors affecting the results.
32:21
So there are various benchmark examples.
32:28
Some of them are called SPARC or the other what is called embassy, which is more for embedded processors, which I'm sure you could just look them up.
32:34
But there are different kinds of comparison.
32:44
So, for example, we could just compare the actual workload because that is what we ultimately care, right?
32:52
OK. But there are also disadvantages for doing that because the workflow is very specific.
32:59
So unless what you try to compute is exactly the workload.
33:07
Otherwise, what you get from the workflow comparison might not be very.
33:12
Entities of particular architecture for instead of the actual workload,
33:17
one could actually look at the whole application benchmark and advantage of that, it is possible and so on.
33:26
It might not be so representative.
33:34
And then there are also benchmark that focus on kernels, which are not very much that covers the sort of peak performance which are easy to use.
33:37
But again, the peak performance of peak comparison might not be the same as the average of
33:48
performance might not be representative of the one that might matter to you.
33:58
OK, so there are all these different pros and cons of these different kinds of benchmarks.
34:04
If you look at a particular benchmark called spec.
34:12
OK, so these are 26 benchmarks which are particular programmes, 12 of them integer programmes and 14 of them are floating points.
34:15
So you see that the integer programme corresponds to a well known compilers like TCC Chess Playing Programme and so on,
34:28
whereas the floating point benchmarks are typically numerical computation because those will require a floating point.
34:37
And then these benchmarks are computer games a particular machine. In this case, it is a relatively old machine.
34:47
Some Ultra five. OK. And they are more recent ones called Sfaxien Use, which started at six and so on.
34:55
And the basic principles are pretty much similar to this particular set of benchmarks.
35:04
And there are different kind of spec benchmarks for Java, for CPU, high performance computing and so on.
35:11
So it really depends on the particular application that a particular kind of processing to select the right kind of benchmarks.
35:19
Otherwise, if you select the wrong kind of benchmarks, the results might not be what you would expect.
35:27
OK. So again, this is just a table comparing different benchmarks.
35:34
So it lists the particular benchmarks, for example, but the canoe compilers?
35:45
And what are the corresponding numbers and so on?
35:51
And then in order to obtain a single number, we just calculate the geometric mean of each of those.
35:57
OK. And you would say, Well, what does it mean?
36:06
Well. That exercise, so you to figure out why the geometric mean is important rather than the arithmetic.
36:10
I mean, I have to explain that in a nose, if you are not sure, you know, we will end on comparing to past intel architectures.
36:19
So we have this Pentium three and four. OK, now if you look at performance of Phenom from Pentium for you, recognise something a little bit unusual.
36:34
OK, so off until three, you'll see that the integer benchmark OK is faster than the floating point.
36:48
Benchmark CFP is the frequent benchmark, whereas 14 the M4 is the 14. benchmark faster.
36:59
So the integer benchmark. OK. Why is that?
37:09
How can they achieve that? Well, it turns out that the way Intel did it is to invent a different set of instructions.
37:13
OK, which I think is code of different kinds of sim, the instructions as this nature or whatever.
37:26
OK, so the theto doesn't matter very much. But the key thing is that when they have this new set of instructions,
37:38
they are optimised for executing those particular set of floating point benchmarks.
37:48
And that is the secret of how they improve the floating points better than the integer.
37:55
So return integer insertion performance to improve.
38:02
OK, but is less dramatic than the floating points.
38:07
OK. So the way that they did is to really go back to the drawing board and design, so they really optimise for doing the floating point.
38:13
But that's how they. OK. So are there any of those questions?
38:22