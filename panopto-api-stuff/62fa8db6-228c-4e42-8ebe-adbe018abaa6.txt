ID: 62fa8db6-228c-4e42-8ebe-adbe018abaa6
Title: L03 - Floating point numbers (part 2)
Category: COMP40001 - Introduction to Computer Systems (Autumn 2021-2022)
Lecturer: Lluis Vilanova
Date: 12/10/2021
Hello and welcome to our third lecture, which is on floating point numbers, so as you remember on the first part,
0:01
we talked about the general theory about how to represent and perform operations with a floating point numbers.
0:08
And in the second part of the lecture, what we're going to look at is how they are actually implemented in computers.
0:16
And first, we're going to start with defining what is the I to floating point standard.
0:24
For those of you who don't know,
0:33
I believe it's the Institute of Electrical and Electronic Engineers and it started in the U.S. But right now they have prisons all over the world.
0:35
And it's an association that takes care of, amongst many other things,
0:45
dealing with organising conferences, but also defining some standards related to computing.
0:52
And in this case, we're going to look at one of them, which is the actual standard for floating point representations.
0:59
So it basically a comprehensive standard that is going to define all the ways in
1:07
which we can represent floating point numbers and also how to operate with them.
1:14
And the advantage of having a standard for floating point operations is that it will allow us to have predictable results independently
1:21
of the architecture so we can execute the same floating point operations on different machines with different architectures.
1:30
But the results will always be the same.
1:38
So the standard basically defines the format of how a floating point numbers are represented, which means how they are stored in memory.
1:43
What are the semantics of the arithmetic operations and also what are the rules for error conditions?
1:52
And one interesting point here is that a few years ago, if you did floating point operations in doing graphics processing units,
1:59
they did not implement the literally floating point standard.
2:11
So when people started using Dubuis for scientific computations,
2:15
they saw that these computations were giving different results compared to the same competition executed in a CPU and a regular Sibiu.
2:21
And that is because if we don't have a standard,
2:35
then we will have different ways in which all the error or error conditions and the epsilon for the computations are handled.
2:37
And that is why having this standard is really important and having it implemented by all the computer architectures that we use.
2:46
So let's start with the single precision format for floating point, which is the representation for 32 bit numbers.
2:55
And in this representation, we are going to have three fields.
3:04
One is one beat for the same. Then we are going to have eight bits for the exponent.
3:07
Remember the scientific notation and then we are going to have twenty three beats for that coefficient,
3:14
which in this standard is called significant Unshorn with the letter F.
3:20
Now what's interesting about this standard is that it's going to use some of that other number of representations that we saw on the earlier lectures.
3:26
So the values that we can represent with this format are plus minus one point F times two to the minus one hundred and twenty seven.
3:40
And the normal beat that one point what f this one point is going to be omitted.
3:51
So it's going to be a hidden field.
3:58
So in fact the single precision representation has 24 beats for the significant with if we count for this hidden bit and remember that seems every
4:01
decimal digit is about three point something beats this allows us to basically represent up to seven or around seven decimal regions of precision.
4:16
Then the normalised renderers that we can achieve with this representation when looking at
4:32
decimal numbers are approximately from minus ten to the thirty eight to minus 10 to the minus 38,
4:38
then zero and then ten to the thirty eight to ten to the minus thirty eight.
4:46
Now, you may ask yourselves why that explosion that we saw here is.
4:54
Two to the power of E minus 127, and remember, I said that we are going to use various representations first,
5:06
I forgot to say that if you look at this, this is twenty minutes for the significant, but there's one separate bid for the same.
5:14
So this is a sign of magnitude representation. And the reason why we have this minus 127 here is because these bits,
5:24
these eight bits and exponent are in fact represented in a bit excess 127 instead of using to complement.
5:33
So this means that the exponent field is going to go from minus 127 when we have all zeros, all the way to 128 when we have old ones.
5:44
The reason why it's represented in this way is that it will allow us to have an non-negative floating
5:57
point numbers to be compared using very simple integer comparisons and we will see why that is important.
6:07
Then we also have a larger representation with twice as many bids, which is called double precision format, which has a 64 bit.
6:17
So again, we are going to have one bid for the same. Then we are going to have from we are going to go from eight bid up to eleven bids
6:26
for the exponent and then we are going to have 52 bids for the significance.
6:36
So this means that these extra bids that we got, this extra 32 bids that we get from when going from 30 to bid up to 60 for which we double precision,
6:41
most of them were put in the significance so that we would have more precision in the numbers that we can represent.
6:51
So in this case, the values that we can represent are plus minus one point F times two today, minus one thousand and twenty three.
7:01
Now, again, the double precision is fifty three beats, because remember,
7:11
there's the one point that is hidden and this is approximately 16 decimal digits of precision.
7:16
So the normalised ranges here are from minus 10 to the three hundred and eight to minus 10 to the minus three hundred and
7:26
eight and zero and then ten to the three thousand three hundred and eight to ten to the minus three hundred and eight.
7:34
So single procedure, once we have double precision with a newer 64 bit,
7:44
architecture's single precision is usually reserved only when when we are in computers that have a very scarce memory,
7:49
which could be some very small embedded devices,
7:59
or when one to when we want to debug some numerical computation, because if we are using a smaller floating point numbers,
8:03
errors are going to accumulate faster and we are going to see larger differences faster in the computation.
8:12
So let's see how we can convert the number into a Jubilee format, and in this case,
8:20
we're going to use the example of forty two point six seven seven five, and we are going to transform it into the Jubilee single precision format.
8:28
So the first thing we are going to do is, of course, convert this number into binary,
8:38
which we already saw how to do in the first part of this lecture.
8:42
Then, of course, we have to normalise the number so we have one point,
8:48
whatever comes after times to do the power of E with, in this case, it's to do the power of five.
8:53
And therefore, we can take this one point.
9:02
And all this part that comes here as the significant and remember everything that comes after we
9:08
just but with zeroes because we want to have all the bits for the significant represented in here.
9:16
Then we are going to take the expert in field, which seems we are in excess 127.
9:24
We have to represented us five plus 127, which is one hundred and thirty two.
9:30
And it's represented like this here in binary.
9:35
And then we can finally take all these pieces and put them into the representation,
9:44
so we take them on the beach for the significant and put them here, then we take all the bits for the exponent and put them here.
9:49
And then we also have to remember to put the zero for the same because this number is positive.
9:59
Now, if we chop this representation up into groups of forbids, then we can write it as this hexadecimal number,
10:06
which is way easier to read rather than all these many ones and zeros.
10:17
And now let's look at how to do exactly the opposite. When I come back, from Tripoli, former to a decimal number.
10:23
So in this case, we are going to start with this hexadecimal representation of the Tripoli number.
10:31
So we would put each of them in here.
10:40
So you can see this. Little squares here that represent each of the first three characters in hexadecimal and the rest are all zeroes.
10:44
So then what we do is we take the exponent field that is here in the representation, which is the represent the number one hundred and twenty five.
10:59
Then remember that this is in excess 127. So the true binary exponent is in fact one twenty five minus one twenty seven, which means it's a minus two.
11:10
And then we have to remember to take the significant field together with the human bit.
11:23
So we have to put that one point in front of the significant field and now we can finally get the value, the unsigned value of this number.
11:31
So it's going to be one point one times to do the power of minus two,
11:43
which once we were presented in binary by applying this exponent is zero point zero one one in binary.
11:49
And then, as you remember from the first part of the lecture, this is going to be equivalent to zero point twenty five plus zero point one
11:59
twenty five for the two weeks that we have here when we're presented in decimal.
12:09
So the number that is being represented here is zero point three seventy five.
12:14
And finally, remember that we have Sambit here,
12:19
so we have to apply it here on the final number that is being represented in this in a single position number is minus zero point three seven five.
12:23
So let's look now at how we perform additions with floating point numbers, and in this example,
12:37
we are going to look at forty two point six seven six eight seven five plus zero point three seven five in single precision arithmetic.
12:45
So. Here we have the two numbers and then we write them down as the corresponding binary representation
12:55
that we already looked at because that is what the computer is going to be actually using.
13:09
And now what we have to remember first is that in order to what numbers in inflow, you know, floating point representation,
13:15
we have to first make sure that the two explanations are the same, which is not the case here.
13:25
These two numbers are not the same number. So what we have to do is take the smaller exponent and make it equal to the larger one by moving the
13:31
decimal accordingly until they are a number of representations that are using the same exponent.
13:44
So in this case, remember that we are in excess and so the smallest number is this one over here.
13:52
And of course, another thing we have to remember is that we have to restore the heathen beat when performing a floating point operations.
14:02
So let's start with that. We are taking the significant from the previous two representations that we just saw.
14:13
For the larger and the smaller number we've put in front of them, the hidden made the one point he made,
14:24
and then we have to compute the difference of between the two exponents,
14:31
which from the two representations that we saw in the previous line are these two numbers.
14:37
And the difference in this case is seven.
14:42
So therefore, this is how many places, how many digits we have to move the decimal point on the smaller number.
14:45
So we do that. We move the point here on the smaller number for seven places from.
14:58
Here all the way to here, some places, and then we can finally perform an addition, and this gives us this significant.
15:08
Over here, just performing regular editions, as we already saw.
15:18
So then the sum is this number over here, the significance that we just got and the representation of the explosion that we took in this case,
15:24
we are taking the bigger one because remember,
15:38
we kind of normalised the two exponents because that's a requirement in order to perform this operation.
15:40
So we take that one, which is five, and we applied to move the comma here.
15:50
So this is the binary number that we got over here.
15:58
And this corresponds to these decimal number that we already learnt how to calculate from the minority representation.
16:02
Once we have that, we can represent it, once we have the significant, we can represent it by simply copying these bits over here.
16:12
Remember that we are dropping the hidden bits, then we are taking the exponent of the larger one, because remember,
16:25
we normalised them, we put it over here, and finally we take the same rate, which is positive in this case.
16:32
So some special values, because remember that there's some additional numbers and numbers that we need to represent.
16:47
So there's some big buttons that have special meanings in this iSuppli representation.
16:58
So in the case of a single precision floating point numbers,
17:08
these special values are plus minus zero, where the SANFIELD can be anything, it can be zero or one.
17:11
So we have two types of zeros, but that's a minor price to pay.
17:18
Then in the significant, we are going to have all zeros and that's what corresponds to a zero.
17:23
Then we can have plus minus the normalised number, which is something that we are going to see later.
17:29
It's four numbers that are very close to zero and they can have a SANFIELD that is either
17:34
zero or one and they are represented with an explanation of zero and any non-zero button.
17:41
And now, even though we are representing this.
17:50
As exponent H0, we are going to calculate it as if the exponent was minus one hundred and twenty six.
17:55
Then we also have a plus minus normalised numbers, which are the ones that we already saw.
18:05
They can have the same rate of zero or one. Their exponent can go from one to two hundred and fifty four and they can have
18:11
any pattern on the significant and then we can also have plus minus infinite.
18:20
So we can have a Sanfield there is zero or one.
18:27
And the way in which we represent these infinite values is by having an explosion of two hundred and fifty five on a significant of all zeros.
18:30
And then finally we have another special type of value, which is the not the number that we will see what it is used for.
18:40
And in this case, we are going to have an explosion of two hundred and fifty five and any non-zero button on the significant.
18:48
So let's see, why do we need the normalised numbers and what they are good for?
19:00
So remember, whenever we have a zero explaining,
19:05
we are going to be either representing zero when the significant is also zero or the normalised number in any other case.
19:09
Then we also have, as I just said. All once on the bottom of the explosion is for infinite and not the numbers.
19:20
And this means that the range of normalised numbers is reduced, so for single possession numbers, the expert in range is minus two,
19:31
minus one hundred twenty six to one hundred and twenty seven instead of the original that we could do of minus one twenty seven to one twenty eight.
19:44
Now, the reason why we have the normalised numbers is that we, as I said before,
19:57
very briefly, we want to be able to represent values between the underflow limits and zero.
20:02
So in this case, we want to way that where our floating point representation is not using the hidden one point representation.
20:10
So in this case, whenever we have normalised number,
20:25
we are able to represent numbers that are plus minus zero point and F times two to the power of minus 126.
20:28
Because remember, this is an implicit exponent because there are actual representation is all zeros.
20:39
And the reason, as I said, is that it allows a more gradual shift towards values that are close to zero
20:47
instead of having a very large gap between the normalised numbers and zero.
20:56
And this is useful in some numerical applications where you want to be able to calculate numbers that are very, as I said, very close to zero.
21:03
So infinity's Anan's. As in this case of the interpolating floating point standard, Infinity's represent values that go beyond the overflow limit.
21:13
So we are going to have plussing infinite and Myners infinite and also it's used for divisions of non-zero quantities by zero.
21:27
So you can do regular basic arithmetic with this infinite so we can have infinite plus five,
21:37
that is still going to always be infinite, then we can do infinite plus infinite.
21:45
That is going to be infinite. For example.
21:50
And then the lines that we saw before are used to represent operations that whose result should not has no real mathematical interpretation.
21:54
So, for example, zero by zero is. Just not the number.
22:07
It doesn't exist. There's no representation for it,
22:13
then we also will have Nunc when we do plase infinite plus minus infinite or zero times infinite or the square root of a negative number, for example.
22:15
And then whenever we have a nun with no break and whenever the result of an operation is a nun, the computer can do two things.
22:30
In fact, there is one thing that it will always do,
22:41
and it's that these nun is going to be represented in the Tripoli flight using the floating point standard and also in some cases on many,
22:43
many computers. That's configurable. We can tell, Disp, to raise an exception whenever the result of an operation is a nun.
22:54
So that will stop the programme. And then the programmer is forced to make a decision instead of silently keeping on going
23:02
performing operations on something that will just keep on being an infinite number.
23:10
So just to recap very quickly, some of the special operations, some of which we already saw and divided by plus minus infinity,
23:16
is going to be zero plus minus infinity times plus minus infinity is going to again be plus minus infinity,
23:28
plus minus non-zero value divided by zero is going to be plus minus infinity.
23:37
That infinity plus infinity is also going to be infinity. And some of the cases that we already saw,
23:42
any kind of zero divided by any other kind of zero because remember we have plus and minus zero in in the standard is going to be a nun, not a number.
23:48
Then infinity minus infinity is going to be a nine, et cetera. So does it really work?
23:57
What happens when we perform these operations on a real computer?
24:05
So here you have a very simple programme in C that basically toys around with the Epsilon, the the the precision of a computation.
24:10
So if you copied this berm into a file and then compile it, we can see here that the numbers that we were comparing.
24:25
Are not equal in the first case and in the second case, these two numbers are equal because of the imprecision of floating point operations.
24:38
So as we said early on in this lecture, we are going to see both cases,
24:50
cases where precision is enough for an operation in cases where it is not enough.
24:56
And the senator is going to tell us that two numbers that mathematically should not be the same for the standard are just represented the same number.
25:02
Also, here's a very simple pseudocode of how to calculate Epsilon of a of a machine for a floating point number.
25:13
And basically we are going to start with one, which for sure is not for sure is greater than the epsilon of the floating point representation.
25:24
And what we're going to do is keep on dividing it by two,
25:34
making it smaller and smaller and smaller until I think this Epsilon or half of
25:38
this epsilon into one is going to tell us that the result is the same as one.
25:43
Once we reach the point, it means that that is the value of the machine's epsilon and it's a representable value.
25:48
That's why we are dividing it by two and then we get that error.
25:57
So here would be the equivalent C-code.
26:01
Demorris, you're going to have this like you can very easily copy paste the code or type it
26:06
manually into your computer and try it out just in case any of you is interested.
26:14
Here would be also double representation for the same programme.
26:21
And of course, if we execute it, that's what an example of what we would get for the epsilon of the machine.
26:26
And finally, if you want to toy around with a special operations,
26:35
here's another very tiny example that you can copy paste into your machine, then compile it with the Jesus come.
26:40
And then when you execute this command, you will see all the results for these special operations.
26:50
And with this, we reached the end of the third lecture on floating point numbers.
26:57
And I hope to see you on the next one. Right.
27:02