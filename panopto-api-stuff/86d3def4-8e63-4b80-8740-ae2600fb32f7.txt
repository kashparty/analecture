ID: 86d3def4-8e63-4b80-8740-ae2600fb32f7
Title: 01731440IteratedGamesProjects3
Category: COMP60026 - BEng JMC Individual Project (Autumn 2021-2022)
Lecturer: Tuan Anh Tran
Date: 23/01/2022
Good morning. This is a presentation of my work on Project three,
0:05
which is implementation of a great learning algorithm for plot to game the
0:10
case where there are three battlefields and each predator has five soldiers.
0:17
In this case, we can show that there are 21 actions possible.
0:23
So yes, I'm sure that the allegations are integers and the actions are listed here.
0:30
I index my arrays from zero. In my project, using this array and the appropriate utility function, we can represent this game in the matrix form.
0:38
This is a payoff matrix for plot entries minus one, zero and one correspond to the case where he loses juice and wins accordingly.
0:54
So in this setting, I was able to implement the actual algorithm.
1:09
I follow pretty much the paper of valour and lock tops.
1:15
At each iteration, I compute the actions based on probabilities which are computed.
1:21
From the regret sometimes, and then I accumulate all those regrets,
1:31
sums and strategies and the quantity strategy some which allows me to then return the average strategy provided on the players.
1:35
It turns out the average strategy profiles converge to it's hard define this year as the sum of nine pure actions normalised,
1:52
and we can show that the perfect hot white hot, which aren't cool, they are still and not equilibrium, and the coach here shows precisely this.
2:04
We show that its heart belongs to the best response to self wrestling.
2:17
That except why have you and us a Cuban? So let's see how that convergence looks like during the simulation training.
2:23
I tracked the Euclidean distances from the NASH, from the NASH agreement to the average strategy profile of Pluto,
2:32
and we can see that we have a sharp decrease in this measure.
2:42
After about thousand iterations, it's around zero point zero two.
2:48
I also experimented a lot if the choice of the initial distributions for the various actions, I tried uniform distribution.
2:54
I also tried to initialise then, not the initial strategies at then another and thus equilibrium every E6.
3:05
I also added some random noise tests.
3:19
In all cases, I got that several strategies converge to X hops, which is pretty amazing in my case and in my opinion.
3:21
We had this fierce resolve that the rest should go to zero.
3:36
And the regrets here are defined as positive regret,
3:40
some sort of by the number of iterations we I tracked this quantity during the simulation
3:45
and we got the confirmation that it indeed go to close to zero in my project.
3:54
I also looked at the Nash Equilibria, the special case where we have two battlefields and the locations are no teachers.
4:01
Blotto has an advantage. In this characterisation. I define only the best probability measures.
4:11
We satisfy two properties. Property one specifies where we allocate the probability masses.
4:20
We put on the masses, where the shaded regions and we neglect all the other regions.
4:29
The property pool specifies how we distribute these masses within those things.
4:36
So this is to ensure that we have no way of improving deviations for the enemy.
4:44
And we similarly define omega e. Not exactly the same definitions, but similar definitions.
4:51
And we have that the pair of strategies from Omega B and E constitute and that equilibrium.
5:00
And we also have that there are no other and thus agree we are.
5:11
So as in my project, I look at the proofs of these forums.
5:15
So that's pretty much the highlight of my project. Thank you for listening, and I'm happy to take any questions.
5:21