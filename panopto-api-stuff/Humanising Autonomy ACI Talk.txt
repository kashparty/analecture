Category: Applications of Computing in Industry (ACI) Talks
Lecturer: Tom Curtin
So, OK, Tom. There we go. OK, so welcome everybody to today's applications of computing in an industry seminar.
0:00
This is the seminar series where you get to learn how companies are using the
0:09
cutting edge of computing to do something really impressive and exciting.
0:14
And it's nice to have that kind of insight.
0:19
I want to say a special thank you to Tom Curtin,
0:24
who works tirelessly behind the scenes to do a lot of the organisation and stuff related to these talks.
0:26
So thank you for that, Tom.
0:34
Today's speaker is Dominic PNoy from humanising autonomy, and he's going to give us a very interesting talk related to computer vision.
0:36
Is that right, Dominic? That's correct. Yes. There we go.
0:47
So when you're ready to please share your slides and take it away, OK, let me.
0:50
I just start like this and then I will. In a two minutes, I will share my slides.
0:56
So first of all, thank you for having me here, Tom and William. I'm very glad I prefer to be in person.
1:02
I think it's much more fun, but I mean, this is what it is.
1:09
And maybe next year we meet in person or in the metaverse somewhere else.
1:12
And so about me, I'm a data scientists from Germany, so I apologise for my weird pronunciation sometimes.
1:18
My background is actually very different. So I I'm coming from psychology, from experimental psychology and from statistics.
1:27
But I worked for the last six, five years in machine learning, and I focus on probabilistic machine learning and payment methods.
1:35
If you've heard of them, I joined humanising autonomy more or less four years ago,
1:44
when you are just a very small company of four people and the adventure which I'm going to show you was just about to start.
1:50
So let me show my screen now. So this is a relatively old photo from a year and a half ago.
1:59
Right now, we are as least twice as many. He has me with those clothes.
2:11
We are about 40 40 something people right now, hiring a lot, scaling a lot.
2:18
Many projects very diverse team from all over the world, maths, background, computer, computer science,
2:24
computer vision, robotics and also social science and psychology HQ2 Somerset House.
2:32
So not that far away. And we are, as William said,
2:38
we are a computer vision company or but people nowadays know what I say to some offensive and AI company with a laser focus on human beings,
2:43
and we build models to understand complex behaviour from that dashcam footage to make intelligent systems more human and more into it.
2:52
Twitter and in certain applications, safer and we built algorithm that capture cognitive robots like the intentions and the risk
3:00
perception and behaviours like social interactions and what is going on in social interactions,
3:09
fighting risks and pedestrian crossing. And such algorithms can be useful for all sorts of applications,
3:14
ranging from manufacturing work to smart mobility sector to virtual reality and augmented reality,
3:20
which is gets recently very exciting, as you can imagine. So today I want to introduce you to two.
3:27
One of the first and biggest projects that we had is that we are working on for four years now,
3:36
and which is to build software to predict pedestrian crossing in order to help autonomous vehicles
3:41
and advanced driver assistance systems to drive safely through pedestrian dense urban environments.
3:49
Um, so let me show you I will show you a couple of videos today, so I hope my internet is fast enough.
3:56
I checked it before, so I hope those videos are going to be interesting to watch.
4:02
If there's something just that, just let me know if it's not possible to see what I'm what I'm talking about.
4:08
But um, it should work. So we we we collected some data just for experimentation, we collected some data from Covent Garden.
4:14
And we were driving around days and days and days the same route and collect data.
4:23
And as you know, driving can be challenging.
4:28
The driver must make decisions about how to use the steering wheel and how to use a
4:33
celebrator to reach the desired destination as efficiently and smoothly as possible,
4:38
and by following apparently traffic routes and avoiding collisions and planning and
4:44
decision making requires here an accurate prediction of the future position of objects.
4:49
Um, this is particularly challenging in such environments. So when driving through a pedestrian dense urban environments due to this huge amount
4:55
of very diverse pedestrian behaviours that could potentially lead to a crossing.
5:02
And it's extremely challenging in places like London because people are so different,
5:06
they're so diverse and the current autonomous vehicle systems like highly autonomous systems, they struggled when driving in such environments.
5:11
They're either extremely conservative, so that driving experience suffers from frequent and abrupt deceleration.
5:20
All the systems are built in an unsafe manner, as has been experienced with the Uber accident in 2018,
5:27
where the safety systems were switched off due to too many false alarms.
5:34
So there's been all know this recent hype about N20 planning, which is very reasonable for a pedestrian crossing prediction and for EVs in general.
5:38
So this approach is certainly very powerful.
5:47
It's not really clear all the time how decisions are made, nor can we obtain reliable and valid estimates of the mode of prediction uncertainty.
5:49
So to make this clear, in order to achieve full autonomy,
5:58
that you could use deep learning and you could use lots of data to build a mapping between the pixels over time and to control inputs to the vehicle,
6:01
which is the steering acceleration, for example.
6:11
And there are plenty of start-ups, particularly in the US, and there are some very popular companies, and you can imagine which ones.
6:13
There's very popular people advocating for it, and they're following such cars, such and when deep learning paths.
6:20
And the general belief is that with enough data and computing power, the problems will be solved at some point.
6:28
And however, we believe that the current performance of those systems and we are not alone on this.
6:34
So the criminal justice system is not sufficiently convincing it their ethical considerations because most systems are biased.
6:39
The big reason is the the data are biased because there's just so much data that it's very hard to control the quality of the data.
6:48
And there are those very interesting adversarial attacks where you wear tiny,
6:57
tiny changes in the image can lead to devasting thing prediction failures and in those models.
7:02
And this I just saying this because I want to highlight that using AI to the deep learning which shows like very promising results,
7:09
but it's probably not the safest approach for now.
7:16
And in order to build autonomous systems that that interact with humans and then the mobility space.
7:19
And on top of this comes that it's hard to understand what what went wrong even retrospectively for, for example, for an insurance company.
7:26
And so the because people have difficulties so deeply not sorry because when deploying networks have difficulties
7:35
making reliable estimates of their own uncertainty and because they like interpretability and transparency,
7:44
most vehicle companies that develop highly autonomous systems and advanced driver assistance systems,
7:50
or ADAS systems, rely as much as possible on traditional conservative but safer techniques.
7:55
And my goal is today. Today.
8:03
To show you what is this safer industry standard?
8:09
Um, then, that the current standard doesn't go far enough. And this is what we claim that you need to model the mind of the pedestrian to some extent.
8:14
So you need to build cognitive models of pedestrian behaviour to bridge that crossing.
8:24
So what is the industry standard, current autonomous vehicles and idle systems able to detect and look like static and moving objects?
8:29
And they can distinguish between the different types of objects. And this many based on deep learning techniques like object detection, stereo vision,
8:38
lighter radar sense of using those systems actually already pretty good in detecting and localising pedestrians.
8:47
And the information that those systems have about the pedestrians look similar to what I'm going to show, you know, in the following video.
8:55
So you can see how a black box that is covering each and that each pedestrian
9:07
that we detect in this image with with an object detection diplomatic technique.
9:13
And this in this in this case, it's YOLO. If you if you know about it and the black box represents what the system knows about the pedestrian,
9:19
which is basically here, the the the pedestrian position over time.
9:28
And based on this current and past position of the pedestrian represented by the black box,
9:32
the future position of the pedestrian can be predicted and based on the predicted positions,
9:38
it can be inferred if the press is going to cross in front of the vehicle.
9:44
And we call this the physics model approach because it uses physical quantities like position,
9:48
acceleration, velocity, information about the infrastructure, UM and the vehicle control movements.
9:54
Um, let me show you how this looks like and more concrete.
10:01
So I'm making, you know, many simplifications to just make my points clear,
10:11
and I assume that at least 50 percent of the audience they may and they may have seen those those graphical model not vacation before.
10:19
So this is a this is a directed acyclic graph or attack and X.
10:29
Excelsior, the true hidden state and why are they observations?
10:36
So this is what we measure and both X and Y index by time.
10:42
Twenty. And the current state. And this you can see in this current state ex is related to the previous state,
10:47
to the previous states by a transition probability density function or a transition table, depending on if X is continuous or disgrace.
10:56
And the observations are related to the hidden states by elected function.
11:04
And we assume here conditional independence, which means that the current state, the current state, is only dependent on the previous state.
11:11
Um, this is what's called the map of property and the current observation is only dependent on the current state.
11:20
So and the advantage of such does a formalisation is that inference over such a mark of change.
11:27
It's also called because of this conditional independence, the advantages that inference over such a lack of training is done in the time.
11:34
So it's extremely efficient. Now. Um, in order to make a prediction into the future, do you have for this predict equation?
11:42
And where you can see here the transition function,
11:53
that's a function before and you have here the posterior distribution over the previous states and to predict the state,
11:56
to predict X of the curve, to predict the state of the current X, we have to integrate over all possible previous states.
12:03
We have to integrate all of those here of the procedure of distribution. And then we have this update equation.
12:11
So then they're coming in. You make new measurements, we make new observations and we update what we predicted and combine it.
12:17
And this is done by like by the combination of the like look function.
12:25
This is what we measure compared to what we believe is the state then said and and and this is the this prediction here,
12:30
and this is combined according to the to the famous to the popular Bayesian Bayesian theorem.
12:38
We have here the probability of your data, which is also quality evidence.
12:43
So this is no update. This is a very generic form.
12:47
Um, let me show you now how this could look in concrete.
12:52
So imagine all that our ways our way is a massive position from a top down perspective in a 2D space.
12:57
So this is the position of a pedestrian and a 2D space from top down from a top view.
13:05
And our ex is now a vector of corresponding true positions of the pedestrian in in 2D and and its respective derivatives.
13:10
So velocity deceleration in both directions. So we have here exosphere represents a vector of six dimensions,
13:21
and now we assume that X is continuous and propagates according to a linear process model.
13:29
This is this very simple model here with a golf course noise.
13:37
Q So this here is just a transformation matrix that transforms the previous X into new X.
13:41
And then we add some noise on top of it,
13:47
which represents uncertainty about whether but actually the position and and and the the other variables like velocity in this race.
13:49
There was some there's also some noise related to it. So under those assumptions, the prediction equation.
13:58
So this is the equation from the previous slide. Yeah, it's just ends up to be just the multiplication of two options.
14:06
Here you have the prediction, and this is the posterior from the previous update.
14:13
So let's look at the way. So if you should know that what I said before that way,
14:19
they observe measurements related to related to X by this linear equation here with with noise noise.
14:26
Ah, well, this is the measurement noise.
14:34
So noise is our measurement to actually detect the position in the in the visual image, then the update doesn't see the update.
14:35
This is the one from the previous slide.
14:44
This is then also multiplication of cautions or combination of caution, according to the Bayesian theory we have here Capricia,
14:46
the common gain and Z the residuals, which is the difference between the extra measurements and our predictions projected into the measurement space.
14:53
So when all those functions in the mall alinea,
15:08
which is here the case and the noise is normally distributed and we iteratively predict
15:12
and update predict the hidden status updates when the new measurement comes in.
15:17
This is known as common filtering and common filtering, as is very popular,
15:22
and it's widely used in different fields ranging from signal processing, robotics, cognitive science and many more fields.
15:27
So such model that I'm showing you here with a continuous state, so we're excellent as a continuous variable.
15:36
It's also called a state based model. Um, we cannot now have a different separate status based model for the pedestrian.
15:44
And for the vehicle. So usually for the vehicle,
15:53
it's more complex because we have control inputs and we have information about the
15:56
infrastructure and other dependent information techniques come into play here.
15:59
But the way for simplification, we have those two models here and you might be able to see, uh.
16:05
So this is just an illustration. I just draw those cycles.
16:13
But this could be a prediction of the pedestrian one frame ahead, two frames ahead, three frames ahead.
16:17
And this ellipse reflects that there's some uncertainty that is increasing over the more we predict into the future.
16:22
Um, and you can imagine that we can cannot we can do no those predictions for the for the pedestrian and for the vehicle into the future,
16:30
for each frame, and then we can combine those predictions in order to compute the probability.
16:38
So [INAUDIBLE] probably do this, that a pedestrian is going to cross in front of the vehicle.
16:43
Um, we call this, you know, here, we call this the physics model.
16:49
As I said before, because we use those dispositions and the derivatives of over time like velocity acceleration,
16:54
sometimes information of the vehicle and pedestrian to predict crossing and where the
17:00
physics model approach has a long and successful history and different application areas.
17:06
This approach has also its shortcomings, which I'm going to present, you know, in a in a couple of videos.
17:11
So please focus now the right side of the screen. So from behind this corner will appear pedestrian and you will see a red frame when the
17:27
physics model predicts that the pedestrians are going to cross in front of the vehicle.
17:36
And so I hope you can see that. I hope my connexion is fast enough.
17:42
So it's a bit flickering, but you should see a flickering frame around the pedestrian.
17:49
And which which indicates that the physics model falsely predicts year crossing,
17:53
and this example highlights that false alarms can occur when a pedestrian is approaching quickly and decelerate slightly.
17:58
And such behaviour is frequently observed when visibility is reduced, costs are calling us with narrow sidewalks as it's the case.
18:05
Saw another example. So again,
18:14
you will see a Rick frame around this pedestrian when the physics model predicts that the president's going to cross in front of the vehicle.
18:21
So and you can see that this turning pedestrian is predicted pretty late.
18:33
And this is very common that you have like pedestrians walking parallel close to the vehicle lane and turn head and shoulders.
18:39
The very first crossing the safe and crossing will follow in such situation if the pedestrian doesn't perceive any risk associated
18:46
with the crossing and physics model doesn't capture this and predicts crossing much later than a human driver would do.
18:52
So let me show you another example. So please focus on all this slowly approaching women who are two or three women from
19:01
the left side and the right frame indicates that the physics model predicts crossing.
19:12
And you will see that physics model struggles with a second woman here right now.
19:18
With the red jacket, and this is very, very common scenario, the pedestrian approaches the vehicle lane, but before she crosses,
19:26
deteriorates and stops and monitors the behaviour of approaching vehicles to verify this crossing is safe
19:32
and pedestrians in such a situation might cross depending on the awareness of the vehicle behaviour.
19:38
Is the vehicle decelerating the pedestrian driver communication?
19:42
Thus, a driver communicate to give way to pedestrians or other other pedestrians already crossing?
19:46
Or is there a traffic jam so that the vehicle couldn't come anyway?
19:52
So also variables that the pedestrian would take into account to compute the risk associated with the crossing?
19:55
And apparently, such dynamics are hard to capture by a physics model approach.
20:02
So. Um, as you could see, the physics model, which bases predictions only on the previous and current distances,
20:11
Strogatz with quickly approaching pedestrians, turning pedestrians, standing or strolling pedestrians and communicating pedestrians.
20:19
Um no.
20:28
In order to overcome those limitations, demonising autonomy model psychological variables directly and explicitly so, we model the vehicle awareness.
20:31
We of the risk perception, we model the intention to cross and and others and combine them then to predict crossing.
20:41
And in order to do so, we must identify the right variables.
20:50
We must understand how they interact. We must annotate data with very abstract labels like intentions, perceptions, risk perception.
20:54
And this is not straightforward to label such data.
21:02
You can't just outsource a tool, for example, Mechanical Turk, and ask other people to do that because those concepts are very ambiguous.
21:05
And if you ask different people what the pension actually means, you might get a different answer.
21:12
And for those reasons, we we have psychologists in the team to build conceptual and causal models of those variables.
21:16
And then in the end of the day of the pedestrian crossing. And we also build deep learning models.
21:24
Many of those, of course, I don't want to be negative about it. I mean, we are a company and we use a lot of deep learning.
21:30
Those are our essential sensors. Without deep learning, we wouldn't be nothing.
21:36
I would say right now, but we use deep learning to extract low level features and not those high level,
21:39
not the intention direct low level features like distances like orientations like the gaze angle.
21:45
And then we infer from smaller models, from statistical models, from probabilistic machine learning models, from Bayesian models.
21:51
We we infer the abstract features which are then combined in further models in order to, for example, predict crossing.
21:58
And now I show you all this looks like, so I refer it to the previous state of space model.
22:07
But because of confidential reason, as you can imagine, I'm showing you is not the model that we are building,
22:14
but I assure you one that is publicly available and which is referencia.
22:21
So this is called switching linear dynamic system. But.
22:27
And the first part. Let's check this out here.
22:34
The first part is the same as the previous model that I introduced.
22:39
So you have a continuous hidden state X and here and this example adjusts in one position to adjust the distance to where the person would cross.
22:43
And velocity. So no laceration just to make it easier to follow.
22:54
So we have this this simple state with which is a vector of 2D, and then we have a linear transition model, which is the same as previously.
22:58
Yeah. To predict the next state of X. And we have an observation status, why conditions on so is the same as before.
23:10
So here you can see the models here, the simple transformation matrix that transformed the X to predict an accent.
23:20
We added the noise to it, and here you have the actual observation.
23:29
This is another transformation matrix that the projects the the the the variable X into that actually what can be observed.
23:32
So, for example, the velocity is only hidden, the velocity is not observed and this is our measured voice.
23:40
So on the right side, here you have the condition probability distributions for the graphical model, which is here, it's just a normal distribution.
23:49
That's the state transition density function and you have here the likelihood function.
23:57
And so this is up to now. There's nothing new except that we have now this stuff on top of it.
24:02
So this year is a discrete switching state variable M, and this has only one of two values with two different motion types.
24:09
So and this is a script switching state selects an appropriate transformation matrix a in order to transform,
24:18
transform the X in order to make the prediction.
24:26
So this matrix can either be so when the pedestrian is walking, which is here the case, or when the person is predicted to walk.
24:29
We have this transformation matrix here where?
24:38
The velocity adds to the next stage, and if the pedestrian is predicted to spend the next day depends on the current position,
24:41
but not on the velocity and Q and are so usually this is more complex,
24:50
but I think this is a good example to illustrate to highlight what's going on, um.
24:55
No, because we conditioned on the switching state. This implies also that the prediction and the update equations.
25:03
So those are the equations which have shown before and were Russia, but I know about that.
25:10
The prediction updated equations are also conditioned on this. Could switching state, as you can see here for for the prediction.
25:14
So the current state now depends on the switching state at the current switching state and the previous switching state.
25:22
And this is here also for the update. It's the same case, so we could say here's another step, which is called the step where,
25:29
which means that we have to marginalise all the previous switching state.
25:38
But I know we can quit going more into detail.
25:44
But most importantly, what I want to make clear is that we can build an entire bank of simple states based models.
25:47
So those models see we can build many of those with different motion models.
25:55
So with different models that transform the current state into another state to make a prediction,
26:00
we can build many different models and then condition them on latent variables like the switching state that sits on top of it.
26:07
And I haven't yet talked about the. So.
26:15
This the UM, the switching state is conditioned on describe a Z and Z is actually comprises a bunch of different variables.
26:20
And here all those different variables which they comprise. So it Z can be factoring into the situation, situation, criticality.
26:29
So how close is the vehicle, how fast the vehicle is approaching?
26:38
The vehicle awareness is the pedestrian seeing the vehicle and has the pedestrian seen the vehicle.
26:43
So this captures if a person is aware of the vehicle that is approaching and then the spatial layout,
26:49
which means is the pedestrian actually had a curve where a pedestrian would usually stop or the person far away from the kerb.
26:56
And those those latent variables determine the switching state that defines which motion model to use in this simpler state's best model.
27:03
And then you have this. This is the likelihood.
27:13
Just like it for the observer, I was given the licence plates and the measure of drivers are, you can see this here.
27:16
This is the distance to the pedestrian I had to,
27:24
but a vehicle to head orientation in order to know if someone has seen the vehicle and the distance of the curve.
27:27
And this is just the the the position of the pedestrian.
27:33
So related to the latent variables, we have those different likelihood functions here, which are different functions of multi gnomeo,
27:37
a gamma, a normal distribution which all permits rise and we have to estimate those parameters from data and those variables.
27:43
So the distances, the head orientation, the distance of this for which we usually use deep learning networks,
27:53
for example, convolutional neural networks. So I'm aware that I rushed here, but um, but some might be familiar with this concept.
27:59
What is really important to understand is that we make those concrete,
28:12
very concrete assumptions about how the different variables are related to each other.
28:15
We make assumptions about what is a process model, what it's like to function.
28:21
What are the parameters? And those assumptions are not learnt from data, but in post by the researcher.
28:26
And then we have some remaining parameters. For example, here, which we have then to estimate from the data.
28:31
And it's crucial to make very appropriate assumptions to not to make biased predictions.
28:38
And this is where we, we believe, expect knowledge about the subject. For example, psychology of the pedestrian behaviour comes into play.
28:43
Um, so our model now follows a very similar approach to what I was just showing you,
28:50
and some areas that we consider is also the vehicle awareness, the risk perception and the intention to cross.
28:56
So what is actually the goal of the pedestrian? Is the person actually interested in crossing at a certain position?
29:03
And those variables are then combined in a certain manner within those states based models that can then can predict crossing.
29:08
And we therefore call this the the behaviour model. And let's see, how do we have a model with no performance?
29:16
And I will show you a couple of videos that to compare to the behaviour model with a
29:23
with a with a physics model offers this state plus model that I've shown you before.
29:27
So you have seen this video before. Um. Just focus here, the woman coming from the left side and.
29:46
The physics model again is displayed in red and the behaviour model is displayed in blue, so when there's this blue frame,
29:56
the behaviour mode predicts that the press is going to cross in front of the vehicle when there's a red frame.
30:04
The physics model predicts that the person is going to cross in front of the vehicle so both can, can be, can be simultaneously, as you can see here.
30:08
And you can see again. So this is what we have seen before that the physics model doesn't predict that Spitzer's going to cross.
30:18
Um, but why? Because she's just standing here.
30:25
And as you have seen before in the states plus model, the state reacts to motion.
30:29
It understands what was the motion, and based on this makes a prediction of where an object is going to be in the future.
30:33
It doesn't make a difference between if that's the pedestrian or another object, and she, in this example she's just verifying, is crossing safe.
30:39
But we already know that crossing to see if she's at the zebra, the car is deteriorating and coming to stop.
30:47
There's a traffic jam.
30:51
So the behaviour model, it captures this and so does a much better job here and predicts Curtis and start that she's going to cross.
30:53
Um, let me show another example.
31:00
You've also seen this example before, turn the turning pedestrian is detected earlier by the behaviour model, then by the physics model,
31:08
and because the model understands that the presence just checking out where the vehicle and the vehicle is already stationary.
31:19
So another example. So here you can see.
31:27
That the president is looking at the vehicle, the vehicle is deteriorating and this is what the behaviour model detects,
31:38
and that's again the reason that the behaviour already predicts at the beginning, just at the start that this president is going to cross.
31:44
Another example. So here's a pedestrian on the left side, and he's he's on a phone and he's not really looking,
31:56
he's pretty distracted, but he follows then another crosser.
32:02
And what you can see here again in blue is that the behaviour kept captures
32:08
this and predicts really pretty quickly that this person is going to cross.
32:12
So, you know, think that, Isaac example.
32:19
They are now two pedestrians coming from the rest left and two pedestrians coming from the right again and blue behaviour model and physics model.
32:24
And you can see that the physics model again is struggling with all four crosses,
32:32
and the behavioural model does a pretty good job and particularly unmarked crossings at slow speeds and residents environments.
32:39
It's not always clear who will yield. Is it the vehicle or the pedestrian? It's been really clear.
32:48
So it's a lot of ambiguity, right? In such a situation, you can usually observe an increased driver pedestrian communication.
32:51
Took this up to disambiguate those situations, and this often results in an active dialogue and active conversation between the pedestrian,
32:59
the vehicle to clarify actually who gives way, why keeping the intersection as quickly as possible.
33:06
And this is actually called a negotiation. It's a negotiation will cross first and for autonomous vehicles.
33:12
It's crucial to understand such situation to know if acceleration is appropriate.
33:18
To say no to communicate, no yielding or to decelerate, to communicate during.
33:22
And our behaviour model can capture when ambiguity increases and the negotiation is about to start.
33:28
So I'm now going to show you some examples of scenarios that happened less frequently,
33:35
but fast crossing prediction would result in a severe unwanted consequences.
33:40
So please focus the pedestrian coming here from the left side is a Red Hat and sunglasses, Black Hat and sunglasses.
33:53
So he's pretty quickly approaching this looking at us and he's actually by his quick approach, he's actually signalling that he would like to cross.
34:00
But you might also be able to realise that the vehicle doesn't decelerate at all, which signals us because at this point,
34:10
the vehicle has the right of way, which signals us that that the person can cross in this situation.
34:18
So if that was an autonomous vehicle using this physics output, this would result in emergency braking.
34:24
Not a heavy one, let me show you a few more of those examples. Here, please focus on the right side.
34:32
You can see a very similar situation where you don't see Blue Square,
34:41
the Blue Square around because the behaviour model predicts correctly that the pedestrian isn't going to cross the border.
34:47
The previous example was the same. So you have to adjust the false cross and prediction of the physics model.
34:53
Let me show you a clip and not quickly another one.
35:00
So same situation, step forward, but the pedestrians wear the physics model predicts that the big that that the press is going to cross,
35:07
why they're behaving more accurately predicts that this president isn't going to cross. So let me show you the last example here.
35:15
This is the most popular one, this is the one from the start.
35:23
And police again focus here at the right side of the screen,
35:29
and you can see here that the behaviour model doesn't really crossing while the physics model predicts crossing.
35:32
And so in those last, for example, the physics model was falsely predicting crossing,
35:38
which is if using such output for decision making would result in an emergency braking and T in the way.
35:43
So I hope you can. You can see the speed. I hope the Wi-Fi is fast enough that you get a notion of that.
35:50
This is actually fast for emergency brake or fast enough for an emergency brake to lead to some unwanted consequences,
35:55
particularly for vehicles behind. And it's certainly not nice to drive a vehicle that is built on such models.
36:03
So here in the video, you might have noticed that the driver didn't break at all, although the presence very quickly approaching the kerb, right?
36:10
The driver didn't break. No decelerated. And why is that?
36:18
So because a driver has so in this case, it's a colleague of mine,
36:23
has a cognitive model of the pedestrian behaviour the driver knows about the pedestrians intention to cross.
36:28
Yes, it's pretty obvious there's this pedestrian wants to cross here.
36:33
But the driver also knows that the pedestrian knows that the crossing in front of the vehicle isn't really good idea in such a situation.
36:36
So. And all those examples, you could observe an interaction between a pedestrian driver and for ensuring a smooth interaction,
36:45
both agents require a cognitive model, often takes a partner to anticipate the actions and to respond appropriately.
36:56
And in contrast to the physics model, our behaviour, our behaviour more captures this because it combines dynamic variables like the cleverness,
37:03
the person's perception of the risk associated with the crossing,
37:11
the intention to cross the vehicle and actually the positions and placements within those probabilistic graphical models.
37:14
And this approach is interpreted interpretable so that we understand the components of the model,
37:22
we in a sense of the limitations of some or at least to some extent,
37:29
and because we compute the posterior distributions of variables and parameters, we have an estimation, an estimate of uncertainty.
37:32
And because of the strongly imposed structure imposed structure by the researcher,
37:40
those models are usually smaller and therefore faster than poor deep learning and models,
37:47
and they can be adapted to different adapt to different environments because it
37:52
doesn't require that much data to estimate the remaining the free parameters.
37:58
And so we can we can present very controlled data to the model to estimate the parameters.
38:03
So importantly, all those advantages that also involve the general status model has the physics model.
38:08
But the physics model, as you've seen in those videos, is not flexible enough.
38:17
On the other hand, those factors are all limitations of poor, deep learning approach.
38:21
So what we are trying to do is we try to combine the best of both worlds by making those additional assumptions about how the model should look like.
38:25
And this very quickly, we back this up with a quantitative analysis showing that we actually can predict up to
38:35
40 seconds in advance if a person is going to cross was relatively satisfying accuracy.
38:43
And so just to finish concluding,
38:50
we believe that probabilistic machine learning models are very powerful to mitigate the limitations of the physics model based approach or fully deep
38:55
learning neural networks by having a more profound knowledge of the pedestrian
39:01
and by explicitly capturing complex vehicle pedestrian detection and makes.
39:05
Our approach can be used when combined with further physical information to make decisions about acceleration,
39:12
deceleration, emergency braking and other means of communication.
39:17
And sorry, I think I'm a bit. It took a bit more time than expected.
39:22
So are there any questions? There we go.
39:29
Lovely, thank you. Absolutely fascinating. I did my Ph.D. in market systems, so to to see it being applied in this way is just absolutely fantastic.
39:33
I have a question maybe just to kick things off and people please have a think about what questions you might want to ask.
39:44
And it's to do with how do you ensure that you've covered all of the case or that
39:53
your techniques cover all the cases that they should because it's working very,
39:59
very impressively in the videos that you've you've showed us.
40:03
But you say this one time in a hundred where this doesn't actually work, this could be terrible, right?
40:07
Especially if the pedestrian has got the right of way.
40:16
And that's obviously a very important thing to try to understand here.
40:21
So how do you systematically explore all of the, I guess, millions of difference of possible combinations or whatever that that could be,
40:25
and therefore make sure that your system exceeds like code must need to be like a ninety nine point ninety nine percent threshold for its safety,
40:36
right? Mm hmm. I mean, this is a very good question,
40:45
as you can imagine that the thing is that we are not claiming that we are able to deal with any kind of situation, right?
40:48
But what we claim is is that we can help the systems to to to to function smoother than than what you can see.
40:57
So can we can exclude certain situations with ninety nine point nine percent accuracy.
41:03
That doesn't mean that we can exclude every situation. And there's still I mean, we still have those.
41:10
We have this Bank of States based models in the background that are running.
41:14
And depending on how many cues we get about the pedestrian, we can exclude certain hypotheses, but certainly won't happen.
41:18
That doesn't mean that we still have this huge uncertainty, right?
41:27
And I mean, because we we update our beliefs about what is happening like in real time on the fly.
41:30
We we, we we we know we have a notion about how uncertain this situation is.
41:37
So but what I would say is a human, a human can't be a human isn't perfect.
41:42
Neither, right? We are not. We don't have a perfect prediction model of all the stuff that is going on.
41:49
But what do we have different than most systems is this very good notion of uncertainty.
41:53
So when we are at night, for example,
41:58
driving at night and it's raining and there are many people around a reasonable driver should slow down because of this notion of uncertainty.
42:00
And this is what we try to do replicate. We are not claiming that we can predict everything.
42:06
I mean, this what I'm presenting here,
42:11
this false second in advance is also very idealistic because it really depends on what the president does, right?
42:12
But to to have this outlier detection of Oh, here is something that is very different than what we have seen before.
42:18
This is what is important. And this like this, this this this this estimate of this quantification of uncertainty.
42:26
And yeah, I mean, it's difficult. It's difficult. We it seems like we can deal with many situations, of course, to give an example,
42:31
a person standing just at the kerb and taking photos of the other side of the street.
42:41
This happens all the time in London. So if we don't model this explicitly,
42:45
we always think that those people want to cross because usually no one stays that close at the kerbside to cross.
42:49
This is one example our umbrellas. You don't see where people are looking.
42:55
We don't see the faces of the people. So we by default, we assume that everyone isn't aware of the vehicle.
42:59
And then by detecting, uh, in in a couple of frames,
43:07
by detecting the head orientation and if if the person that looked at the camera vehicle by taking this and a couple of friends,
43:11
it increases the probability that someone might be aware of severe because it still doesn't.
43:19
We still have to combine it with how quickly the pedestrian approaches and at certain point,
43:23
the model would say, Look, he's that that person is still walking.
43:30
That doesn't make sense. We haven't seen that before. It's better to to to highlight now that this person might still cross the street.
43:34
So it's it's very complex. It's very difficult. Yeah, yeah.
43:41
I mean, I liked what you had to say about, you know, there's almost an implicit negotiation,
43:44
isn't there based on body language and gestures and things.
43:49
And they're quite subtle between the driver and the pedestrian ran into say, I want to ask something else which is around.
43:53
So how who do you who are your customers? I mean, do what manufacturers of driving, you know, self-driving cars?
44:00
Would they kind of use your software and install it as a service in their in their vehicles?
44:08
And if that is to be the case, like where do you draw the line with respect to the liability that you would you would accept for,
44:16
say, an accident arising out of, say, the failure of your? Behavioural model to predict that somebody was going to cross, maybe.
44:24
Mm hmm. That's a very good question. I'm I have to say I'm the person who does the modelling.
44:32
I don't know really well what I am allowed to say was what is how to proceed official,
44:37
who are our customers, but I can can talk about some of our customers I can talk about.
44:44
We are collaborating with Daimler with Nissan. So usually that they have their their RV departments, right?
44:48
And what we what we deliver is this prediction based like wrapped up in an SDK and this is then combined with the software stack.
44:56
So what we don't do is, for example, we claim that we are the human experts.
45:08
We claim that we are good in understanding those different variables about the human.
45:13
We don't claim that we have really good segmentation, that we are really good in understanding,
45:16
whereas the kerb that we detect everything of the infrastructure, that we are really good in predicting the path of the vehicle.
45:20
This comes from the from the from from from the company. And this is then combined with our this this information, we get them from them.
45:26
We combine it and then we give something back. And so we are not doing the we are not doing the we are not working on the decision making itself.
45:33
I mean, of course, I worry about it, but the the final quality insurance is done by the by the by the manufacturer.
45:41
OK, great. Go ahead. Anybody else have a question? Yes, I'd like to ask the question.
45:50
Go ahead. So I am wondering, so basically you showed us a lot of videos from London, I believe.
45:57
And I'm just thinking, for example,
46:05
I've been in India once and the roads are obviously quite chaotic there and you've got people crossing in front of cars all the time.
46:07
That's like traffic jam. And yeah, I wonder if there's like also cultural differences involved in modelling that behaviour,
46:15
because I imagine that the London model's not going to work well in,
46:23
for example, a developing country where people don't really behave like road rules and that kind of stuff.
46:28
And I just wondered if you've had any research that direction or is there a very good question?
46:36
And it's a bit was how we are selling us.
46:44
We are selling us as a company that tries to understand those different kind of behaviours and in different contexts.
46:46
And we don't just we don't train a model in one context and just apply it to something else.
46:51
Of course. So you're absolutely right. It's different.
46:57
The same model to a certain extent wouldn't work in a in a different context, and you feel you provide this example with India.
46:59
I have to say, though, that London is very difficult because it's diverse, right?
47:06
Because you have people behaving in a very conservative way.
47:11
They're very careful and then you have other people who don't care at all.
47:15
So I, to a certain extent, I believe that that many things that we're developing works in other contexts.
47:20
But and now now coming back to this approach that we are following as we rebuilt it in a modular manner,
47:25
what we don't have this huge network that learns end to end from the Pixel two to crossing prediction we built what is intention to cross?
47:31
For example, this is one module, and we built this based on labour data of where people were labelled of, if that what is the intention to cross?
47:40
Then we have another one, which would be a model of situation criticality.
47:49
So how how critical a pedestrian thinks the situation is.
47:53
And those kind of modules that are then later combined and then retrained.
47:59
But those kind of models, they can be refined.
48:03
We can see which model is failing and they can be refined based on cultures are position specific data or demographic sites.
48:08
So we can be. For example, if a pedestrian is aware of the vehicle, I assume is relatively independent of the culture.
48:17
It's mainly based on where is the pedestrian? What, what is the distance to the vehicle?
48:25
How many exclusions? What is the velocity of the vehicle?
48:30
Maybe. Right? But it's relatively culture independent. While situation criticality,
48:34
which I define as that's a pedestrian perceives crossing in a certain situation as risk or not is highly culturally specific.
48:40
So what what what we try to do is we train specific models of the model based on
48:46
culture specific data on environment specific data while keeping the other ones fixed.
48:52
Um, yeah, yeah, I mean, it's certainly true that in different countries and so people have very different respect,
48:58
for example, somebody waiting at a pedestrian crossing to cross the road, right?
49:05
Totally. Totally, and you can imagine, for example,
49:10
to detect if someone is interested in crossing and in certain places, you have everyone lined up at the kerb.
49:13
In other places, you don't. It also depends on how many presidents are there in the first place, but it is very easy to model.
49:20
It's very easy to model. And Munich, for example, we have some data from Munich.
49:28
It's easier to model in Tokyo, although it's a big city, but people are following much more the traffic rules.
49:32
It's hard to do it in London, I believe, and in places like you said, India depends on which city.
49:38
But it's almost easier if, if, if because the traffic is much slower.
49:44
Many more people are basically. It's much more robotic motion planning problem than actually and to consider
49:51
those variables that that that I was presenting you in those environments.
49:58
Very interesting that answers the question of Bushra, thank you. Great, anybody else have a question?
50:04
I mean, if not, maybe you could just tell the students a little bit about what kind of opportunities are available for them with humanising autonomy?
50:17
I mean, do you take placement students? You looking for graduate developers?
50:25
Are you happy to supervise or help supervise student projects?
50:30
All of it. Actually, where we are placement is all right, we're looking, for instance, we have right now to from Empire.
50:38
They had stated a place with us. I think a year ago, two years ago, and they had just joined us and we are extremely happy with them.
50:45
But we all. Yeah, so. So all of those situations, I'm not 100 percent sure about seasons like DOMA does.
50:54
I would need to double-check, but I think they are open for everything, actually.
51:03
So what we are looking for is we we have we have those research scientists,
51:07
we have people who are actually doing the development and we have to deploy people to deploy.
51:11
And then we also need top engineers.
51:16
We need to put engineers, computer vision engineers, deep learning engineers, just research scientists were good in many different fields.
51:18
So please just reach out.
51:28
And I mean, what is very important is today I presented a use case for autonomous vehicles and we are working on this for years,
51:31
as I said at the beginning. But we we are doing many different applications and particularly we're very excited to go into this,
51:38
into this virtual reality space right now to understand social interactions and things like that.
51:45
Was it hard to drive your car safely in the metaverse? Yeah.
51:52
It's interesting where humanity goes. I get worried about it sometimes, to be honest.
51:57
That's fantastic. Thank you. Thank you very much, Dominic.
52:04
Any any last question from anyone. Now, if students are looking to get hold of you, Dominic, what's the best way for them to do that?
52:08
Reach out to me directly. Yeah, I mean, like an email or Twitter or something.
52:23
That's good for that too. Yeah. Email, email or LinkedIn.
52:28
OK? Yeah, great. I think Tom has your details, right?
52:33
So anybody who's interested in just email Tom and pass those details on.
52:38
OK, well, thank you very much, Tony. That has been absolutely, very,
52:43
very interesting and thought provoking and a great exploration of all the sort of
52:47
ethical aspects and stuff around the the problem as well in the legal aspects.
52:54
So this was fantastic. Thank you very much. Happy to hear that.
53:00
Thank you. Thanks, everybody. Just as.
53:04