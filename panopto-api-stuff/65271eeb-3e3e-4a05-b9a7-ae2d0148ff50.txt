ID: 65271eeb-3e3e-4a05-b9a7-ae2d0148ff50
Title: 7.4 Analysis
Category: COMP40008 - Graphs and Algorithms (Spring 2021-2022)
Lecturer: Iain Phillips
Date: 27/02/2021
So let's look at the worst case analysis for quicksort. We've seen that there are always N minus one comparisons.
0:01
Supposing that we had the list where the left home based element is warm.
0:10
And then we we do this spitting around watton.
0:16
Well, then the list is going to leave everything over on the right hand side after doing the comparisons.
0:21
So the only thing that is now incorrect position. Well, got the one incorrect position.
0:28
But we still have to recursively sort this list of lenth six.
0:33
So we've gone down from a list of land seven to a list event six at the expense of doing the six comparisons.
0:38
So that's actually a bad case for quicksort. What we want is a roughly even split so that we're halving the size of the list on each side.
0:45
But this is a bad case where the split element ends up basically in in its it hasn't moved.
0:54
And we have got a large list to sort on the right hand side.
1:02
Well, if you think about it, a particularly bad case would then be where the L is already sorted.
1:09
So in that case, we can go ahead splitting around the one and then around the two, etc. and we have to do that six times to get down.
1:15
And this turns out to be, in fact, the worst case. So we can write down a recurrence relation for that.
1:28
Obviously, if the list is just lenth one, then quicksort needs to do nothing.
1:35
But otherwise W N is N minus one for the non recursive work done in the splitting plus W of N minus one.
1:40
We've just got the one half the one side to recursively apply the algorithm to.
1:50
Well that's easy to solve. We can see that the W of any is just a minus one plus and minus two all the way down to one.
1:58
But and which we know how to solve. That is just an times N minus one over to arithmetic.
2:09
Serious. And this is no better than insertions thought.
2:15
In fact, this is precisely the worst case performance of insertions sort.
2:20
So quicksort can be quite bad in the worst case, which makes you wonder why it's called quicksort.
2:25
And the reason why quicksort is good is good is because in practise, the split is unlikely to occur.
2:34
Why do you have to be unlucky to get that split? And the average case performance of quicksort is a strictly lower order than the worst case.
2:42
So let's have a look at what the average case number of comparisons might be for quicksort.
2:52
Well, we need to see what could happen after the splitting is is done.
2:59
Well, the split position can be anywhere from one to N. If we index the the array from one to end here.
3:05
And as as we saw a bad case would be where s was actually equal to one.
3:14
But we've got all the possibilities for S. Could be anywhere from one to N.
3:20
And if and if we split the split position ends up being at S.
3:25
Then we have a list of lenth s minus one on the left here. And a list of Lenth and minus S on the right.
3:31
And we apply quicksort recursively. So that will take us a of S minus one plus a N minus S.
3:38
Comparisons in average case. And so we can sum all these up a Aveion is N minus one for the non recursive work.
3:45
The number of comparisons plus summing up all of these possibilities times the probability of each of these possibilities.
3:55
And without knowing anything more about the list, we can assume that each of these split positions from one to N is equally likely.
4:04
So let's assign a probability of one over N to each of them.
4:12
OK. So that gives us this recurrence relation.
4:16
And we can simplify that slightly. And we get this recurrence relation.
4:22
A if one equals zero A and is N minus one plus two over N times this sum from I equals two to N minus one of a.
4:28
I the reason being that as you see is s varies from one to N then this will vary from zero up to N minus one.
4:39
And this will vary from N minus one down to zero. So they affect effectively each term occurs twice, giving us this factor of two.
4:47
Well, unfortunately, this is still quite complicated for us to solve.
4:56
So I'm not going to do that here. And I'll refer you to Belser and Gildas book for the proof or indeed and many other algorithm books.
5:00
And but it turns out that average case and is actually order n log N, which is the same as for merged thought.
5:09
So and is basically the same as the optimum.
5:19
So average case is good for quicksort but not worst case.
5:23
Well, if you apply quicksort in the average case, we can actually compute for an equals 10, for instance.
5:30
We can see that the average case lower bound we had before with twenty one point eight four.
5:39
If you compute a van for quicksort just using that recurrence relation and solving it.
5:44
Exactly, you get twenty four point four four, which is not too bad.
5:49
And for comparison, in the worst case, lower bound for an equals 10 would be 22.
5:54
And the worst case for most thought would be 25. So therefore, quicksort, an average case is just as good as merged thought in worst case.
6:00
But hang on. That might would appear to mean that merged thought is better than quicksort because merge sort is guaranteed to have good performance.
6:11
Whereas quicksort is good on average but could get bad.
6:20
But the reasons why quicksort might be preferred to merge thought in practise.
6:25
Well, you can improve the chances of a good split. You've got things you can do in quicksort.
6:32
So for instance, I said that we have to juice the leftmost element to spit around, but he doesn't have to be the leftmost element.
6:38
We can choose some other element to split around. So, for instance,
6:48
sometimes what people do is to randomly select three values from the list and then take the median value and
6:52
use that to split around as being a sort of better estimate of where the middle of the list is likely to lie.
7:00
So you can do that kind of thing with quicksort, whereas Merge sort is a rather inflexible algorithm that you just divide in half.
7:08
And you've got no control over how you do that division into the half.
7:16
It just has to be done that way. The other reason why quicksort might win out over Merge thought is that quicksort is an in-place algorithm.
7:19
We've seen that when we're doing the splitting. All we're doing is actually swapping elements around within the list.
7:29
So no extra space is used, whereas when you do the merging, you use extra space to write out the merged list.
7:35