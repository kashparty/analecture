ID: 13812aae-7beb-4505-9a2f-add800bc0584
Title: Advanced Programming - Knot tying
Category: Haskell
Lecturer: Anthony Field
Date: 06/11/2021
Yeah, so I guess, can everyone see the screen and the text on the screen?
0:03
OK, cool. Awesome. Thanks. So yeah, so what's your role in first advanced programming lecture?
0:09
And you know, as Halloween season is coming up, I thought, Oh,
0:17
let's write some spooky high school programmes because it's always it's not tying
0:21
in particular is always something that scares people when they first see it,
0:28
so. Perfect timing. Now, in order to, you know, let's warm up a little bit at the beginning.
0:33
So let's talk about a friendly number of sequences called the Fibonacci numbers.
0:39
These are the Fibonacci numbers, and I'm sure most of you are have at least heard of this sequence.
0:45
It's very simple. It's it's a sequence of numbers is generated in the following way, so it starts with two one.
0:51
The first settlements are one and one, and then each subsequent element is the sum of the previous two elements.
0:57
So one plus one is to one plus two is three.
1:04
Two plus three is five. That's okay.
1:09
So, etc. That's how it goes. This is the Fibonacci sequence and just that description.
1:13
I gave you an English of how it's generated. We can implement it in a simple Haskell programme.
1:19
Very much the same way. So I say, you know, the zero element is one, and the second element is also one.
1:28
And then the end element is generated by, well, it's fib and minus one plus and minus two.
1:37
All right. So this is exactly the same programme, and now I can load this into JCI.
1:45
And we can you said one is one fence, three is three, five, it's a etc.
1:54
So, you know, pretty simple and friendly Haskell programme.
2:03
Now the trouble with it is as you start asking GHC to evaluate Fibonacci for larger numbers, let's see what the twentieth number is.
2:07
OK, well, the 30th number, it becomes very slow and in fact, it becomes exponentially slower as we go now.
2:22
The reason this is the case is because when we evaluate, you know, let's go back to the definition here.
2:34
When we evaluate the N3bn. Actually, no, we need to evaluate the end minus one and the end minus two Fibonacci numbers.
2:44
But the computation for the end minus two also includes the computation for that's the competition,
2:53
as one also includes the computation for and minus two and minus two gets computed twice.
3:00
And actually, we can illustrate this. I can import this evil module called Be-Bop with a trace.
3:07
So I write Haskell for a living as well, which means I use a lot of these dirty,
3:13
impure side effects for functions are not going to jump too much into how that works.
3:20
Just kind of illustrate. It allows us to do traditional print style debugging so I can say, let's trace computing thin.
3:25
Show and. It was a little bit slow going to trace this to illustrate, so can predicted five,
3:38
you can see that the amputation of 55 invokes a computation of three,
3:50
which it invokes the composition of two and then that branch is done then without it, for which then compute.
3:55
Step two also computed three, which then Compute Stick two and the, you know, the last two cases, we didn't put the trace function there.
4:05
So those are not printed, but also only two. And this goes is gets even worse as the number goes up.
4:13
In fact, it grows exponentially. So perfect eight will compute Page Six, which does a bunch of work.
4:23
And then we compute Page Seven, which does a bunch of work, including computing.
4:30
Tip six again, all the work is duplicated. So because of this, I'm not going to use Debug Trace anymore, so I was just little.
4:34
Little Segway, but yeah, very slow.
4:46
So one way to make this function faster is to give up on some of the going, you know, look at you, look at this function and it's it's quite nice.
4:51
It's very simple and very directly describes what the Fibonacci sequence is.
5:01
Unfortunately, it's very slow, so we can trade some of that readability and get more performance.
5:08
So one way of doing this, as the name of the next function suggests, is to write a tail recursive.
5:15
Fibonacci sequence, a tail recursive implementation,
5:24
this function and this approach is called sometimes called the bottom up dynamic programming approach.
5:27
The idea is that instead of recursively calling the sub computations,
5:34
we can observe that actually we if we know the last two numbers, we can always compute the next one.
5:39
And that's precisely actually how we define the Fibonacci sequence.
5:47
So if we start at the beginning and keep going to the right one by one and only always remember the last two elements,
5:51
we don't need to duplicate any of the previous computation because we have the previous two elements in hand.
6:01
So I'm going to do this by implementing it with a helper function, which I'll call go.
6:08
So Go will take the number, and initially it's the original number and the first two elements of the Fibonacci sequence.
6:15
OK. And then this is a helper function go, which takes, but I'm not going to write the type signature.
6:24
It takes three integers and returns in the future. So in going zero, this is the base case.
6:30
It means we're interested in the set of the first element of of what we remember.
6:36
It's f one an f to return f one. Now when go is sorry, when the argument to go is not zero, then we take a step to the right.
6:42
So we call go recursively with the argument decreased by one.
6:54
And so we shift the numbers f to goes to the left.
7:02
So the next number is up to. And then the new numbers f one plus f two, right?
7:06
So this is exactly that one step in one step to the right that we that how we describe the people, not your sequence.
7:11
So ask Pigtail 10, fifty one hundred fifty thousand ten thousand and.
7:20
It works for really, really big numbers as well. And in fact, this is.
7:32
It's it's so big that, you know, I don't even know how to pronounce it.
7:39
Let's see. OK, has that many digits.
7:42
So, OK, so big is way faster, but if you compare the look of these two functions, fabled tip tail feathers, you know, very easy to understand.
7:50
Very. Easy to explain what it does is.
8:01
Well, it's, you know, something it looks a bit more industrial. It's a it's it is what it is.
8:07
It's that you have to sit down and look at the indices and figure out what's going on.
8:13
So the question is, can we somehow? You know, get some of that speed.
8:18
But retain the nice look and shape of this original function.
8:25
And that's exactly what we're going to do next. But before that,
8:32
I actually just wanted to make a quick note about why I'm so particular about using the integer type instead of the Haskell Sardines type,
8:35
which you may or may not have seen this function with type check with the type as well, but it wouldn't do the right thing.
8:44
And here's why. So small numbers is right.
8:53
But if you the moment you start to go bigger, you get these strange results.
8:57
And the reason this is the case is because integers are actually sort of the int type is represented in memory as a fixed size, 64 bit integer.
9:02
In fact, it hasn't as a largest element, whereas integers the integer type is arbitrarily large.
9:16
It's you can they can represent arbitrary, really large numbers.
9:27
And in fact, if I ask for what's the maximum integer, I get a typewriter saying, Oh, integer is not bounded.
9:34
So that's precisely what that means.
9:40
And you know, it's especially important for playing around with Fibonacci numbers because they get big, very quick.
9:42
So. So that's why we use integers and there's a side note.
9:49
OK. So. Right now, a common programming practise when you want to make computation faster,
9:55
especially when you want to avoid reading a lot of computations, is to use a cache of sorts.
10:06
So use a data structure that holds previous computations. And you know, initially you want to compute that the element.
10:15
But once it's computed, you don't want to redo the computation, you want to just look it up in the in the table or in the cache.
10:24
So this is sometimes called memorisation. Now let's. Let's say that we don't want to.
10:32
They want to have this pig tail function, just keep a simple one.
10:39
Now I'm going to start by actually, you know, how do we go about building this stable?
10:43
So let's build a big table.
10:48
So the table is going to be a cast of all the Fibonacci numbers and we'll start by, you know, just very naively build a list.
10:50
Of all the Fibonacci numbers, and, you know, so this already works because we have an implementation of the people in action numbers here.
10:59
And it is an infinite list. But because Haskell is lazy, it actually won't compute the list only up to the point that we need, right?
11:08
So I can do a take five said table. And that's the first site they would actually offer that can take 10.
11:19
You know, that's a step. So the list is not evaluated.
11:27
It's just it's it's only when we sort of force the list by functions like take that, it gets evaluated so, you know, infinite distractions like this.
11:33
Are perfectly fine in high school, so if you want to do a, you know, first step would be let some you know their prime.
11:44
And instead of doing a recursive step here, we might want to say, OK, let's look it up in the in the table now.
11:59
Of course, we have a problem here because the this is a type error, because the bang-bang function,
12:07
you know, the look up an element in this function in the Prelude takes it in.
12:15
But we are dealing with integers, so we need a version of that that that deals with integers.
12:20
So I'm just going to really quickly really find the missed index function.
12:27
In order to do that, I'm first going to hide the one from the predicted Oops.
12:31
So we can say, I don't want I don't want the bang-bang function from the Prelude.
12:37
Instead, I want to define it myself to work on integers.
12:42
You know, and. Ex-cons, so, so so when we're looking at the zero index just returned the first element.
12:49
Otherwise we'll we'll do a recursive step here. So access.
13:00
Okay, so that's how that function is implemented. And now this function type checks.
13:06
So fed prime. Now looks things up in this table, but there's a bit of a problem here because.
13:11
Even though they prime no longer does duplicated computations, it still relies on the table that was built in a very slow way.
13:21
What I mean, if I try to do Prime 100? It's still really, really slow because in order to compute the cache, we use this very slow function.
13:33
Right? So it looks like. We didn't gain too much.
13:45
It's just sort of one of these. So I'm going to interrupt this because it's never going to finish now.
13:51
Only if we had a way to compute the Fibonacci numbers efficiently up to the point that we need them for the Fed prime function.
14:01
And if you look at the Fed prime function uses the lookup table, but it only ever uses previous elements.
14:11
So you know, the prime of RN will refer to Fed table at end minus one and minus two.
14:20
Or, in other words, if you just use Prime itself to populate this table, then then we can actually make progress by kicking things off.
14:28
So let's see if prime of 10. OK, so that magically works, and now if you look at this little bit,
14:44
it looks quite magical because, you know, we have two definitions that are mutually recursive.
14:57
They depend on each other. Its food table is built by Fed Prime.
15:04
And Fit Prime is built by big table, but thanks to Haskell's laziness, it actually doesn't care that it doesn't exist yet.
15:09
It just says, OK, well, you know, the moment you start asking about the elements of this list, I'll start evaluating the other ones.
15:17
But you know, I won't bother until that's necessary.
15:26
And the moment you start asking about the indices, well, you know, how is this list built up with the first 10 elements?
15:29
Oops, that's the wrong one. Take 10 of the table.
15:36
So you know the first element? Zero. We'll call the prime of zero.
15:43
We know how to compute that. That makes no reference to the data structure and frame of one.
15:48
The second element also makes no reference, so we can already build the first two elements without, you know, depending on previous values.
15:55
And then finally, the recursive case, we will start actually looking back into the cache, but only the elements that are already computed.
16:05
So in the case of type two, this will be the zero element and the one element so that we know that thanks to Haskell's laziness,
16:13
that's just about enough information that we've already forced.
16:23
So. Yeah, so like I said, thanks to laziness in high school,
16:28
we can actually build these sort of mutually recursive data structures and mutually recursive computations.
16:34
And it's like it's kind of like tying the not together because you have all these loose ends and then at the end, magically things just work out.
16:45
Now. I think that's pretty cool question from the Shenzhen.
16:53
Yeah. I have questions.
16:59
So when we use phone phone for the index, how we actually raised the time complexity because actually take more time to index the whole list.
17:01
Yeah, yeah. So that's a very good point. So if you remember what I said about fit, I said it's excellent.
17:13
Maybe I didn't say, but it's exponential complexity.
17:20
Tails complexity is linear because it just it just, you know, iterate through until it reaches a point.
17:23
It's strictly linear and 50 primes actually quadratic. So that's that's true because for each element, you have to do a look up.
17:31
And it's always sort of the worst case because you always have to walk the whole list and look at the very end that you've computed.
17:42
So it is true that, you know, the original one is this is exponential.
17:49
This is linear. And this one is quite dramatic.
17:58
So, you know, obviously is great quadratic is worse, but it's way closer to linear than exponential.
18:05
So but but indeed, if you start, it's a very good point.
18:15
If you start, you know, big tail a thousand finishes really quickly, a prime a thousand finishes a little bit slower.
18:19
And you know, obviously because it's quadratic, the slow down will will grow as we as we look at larger numbers.
18:30
But it's a little bit closer to, well, you know, it's the function itself looks very similar to the original one.
18:39
So it's kind of a it's a little bit of a trade-off. And of course, I'm not using these examples.
18:47
As you know, here's how here's here's the best way to implement the Golden Triangle,
18:51
although I will show you the best way to go and see what actually numbers in a minute.
18:56
But these are just to kind of give you a flavour of. Recursive lazy computations in high.
19:00
But, yeah, very good point. Time complexity, why this is a little bit worse than the recursive version.
19:07
So. Oh yes, I point about this. Does anyone want to make a guess what would happen if I.
19:14
Because, you know, right now the table is a top level thing. And what would happen if I move it if I move into a helper function here?
19:23
And so this is the same exact shape of the programme.
19:37
Does anyone want to make a guess what happens when I run? But prime now?
19:42
Yeah, I see some hands up so special. Brian Neal think he was just.
19:53
I just creates the table each time from scratch.
20:00
Exactly, exactly. This is a very, very subtle point, but what actually ends up happening is it'll once again be very, very slow.
20:04
And in fact, it's probably going to be even worse than the original one because.
20:13
And when in fact, it was a top level thing. But the its contents are shared between all the computations.
20:19
But when I put it into this local helper definition, which is a completely reasonable thing to do, by the way, because you know,
20:27
you look at it and like, Oh, I don't need this table hanging around, I'm just going to stop it into this local scope.
20:34
So it doesn't, you know, it's not a no one's business, what's going on in here,
20:40
but actually what that means is for each fed prime and computation, a brand new table is going to be built.
20:46
There might be, you know, you might be thinking, OK, C could do some clever optimisations where it notices that actually there's
20:55
no reference to any of the local variables here because it's two completely,
21:02
you know, it's a it could be lifted to the top level and C might do that optimisation if you compile it with the dash over to fly.
21:07
But you know it's better to not rely on optimisations in this way.
21:16
So that's why this table has to be a top level thing so that it is shared between the computation.
21:21
So we only build one one perfect step table instead of the quadratic amount.
21:28
So yeah, that's that's fine. Thanks. OK.
21:35
Now I said, like I said before, I am going to going to show you the best way to build the Fibonacci numbers.
21:40
But in order to show that, let's recap a few things about Haskell Haskell's infinitely disrupters.
21:48
So here's a very simple one which you might have seen.
21:56
So if I want to generate a list of infinite numbers, sorry, an infinite list of numbers, let's say that I want an infinite list that holds just one.
22:01
So each element is the number one. I can do that by saying one comes onto the list ones.
22:11
OK, so this is a list of integers I can type.
22:19
So I can evaluate this and it would just go infinitely. I can take 10.
22:27
The first 10. Elements of this list just gives me back the first, you know, one 10 times, so it's a very simple way of building infrastructure.
22:32
And whenever you build an integrated data structure, the only the main thing you need to worry about is am I making progress, right?
22:45
Is there sufficient information at each step to progress forward?
22:55
Now, if this one Collins wasn't at the front of the list, then?
23:01
We're not making progress, we're not producing new values, and we'll just hang indefinitely.
23:07
So let's look at a little bit more complicated one. So this is just all the numbers.
23:14
So I mean, let's look at the sticker generating all the natural numbers starting at maybe zero.
23:21
So you have this list. Zero, one, two, three, four, etc. Now we can look at this and spot that, actually,
23:28
this also has a recursive structure because this is the same as what happens if you add one to each element.
23:37
Here you get one to three four.
23:46
Five and six. So if you add one, you know.
23:51
Must suck that thing up there, the successor function, then you get this.
23:57
So if you cons zero at the beginning. Then you get back to the original list.
24:04
So actually, we can do exactly that.
24:13
And that gives us the first 10 natural numbers, but of course, this is an infinite list, so it'll just keep going on until interrupted.
24:21
This is another one of those infinity structures that makes progress so it can produce an infinite number of an infinite number of values.
24:29
So after this little Warm-Up, let's go back to the Fibonacci sequence here and try to see if we can play a similar trick to,
24:42
you know, build an infinite structure Fibonacci numbers by spotting some relation on on the numbers.
24:54
And it's not that difficult to spot because we, you know,
25:02
the way I introduce the Fibonacci sequence is precisely by giving this relation on the numbers.
25:05
But in order to see that from the sequence, we have to massage it a little bit.
25:10
So what I'm going to do is first, take the tail of the list above. And so, if so, this question from Richard, if you want to have it, yeah,
25:17
go for it degree, just go over the map, suck zero the the functioning wrote before this.
25:25
Again, that's yeah. Sorry, what's the question?
25:31
Could you just go over knots again one more time? Could you explain that real quick?
25:37
One more time so. Oh yeah, yeah, yeah, of course. Yeah. So the idea here is that the infinite list of numbers looks like zero one two three four five.
25:40
And your observation is that if we know part of that structure, you know?
25:50
If if we know. If we know that the.
25:59
But that's what we're trying to do here is kind of find a way of defining the natural numbers in terms of themselves.
26:07
And so the particular way I found here was that I say, Okay, well, if I'm one.
26:14
To each natural no. Then what we get is is is this two three four six four five six.
26:19
And so that's you know, if this is if this is a natural numbers, then this is not suck nuts.
26:33
Now. There and then the point I was making is actually.
26:42
In order to reconstruct the original original list, we can just pretend zero.
26:49
And this kind of magically works out zero icons, maps, suck maps.
26:57
This magically works out because every time you're asking for the next element.
27:01
It was shifted the whole list to the right by one, so every time you're asking for the next element,
27:08
you're only ever going to need what was previously computed.
27:15
And that's it's kind of this is like this.
27:20
Yeah, this is the spooky bit that I was talking about because it looks like it shouldn't work.
27:24
It looks like this should just blow up. And you know, it's this should be legal.
27:32
But, you know, in high school, thanks to laziness and it's a bunch of other black magic, it works out.
27:38
So yeah, and we're going to do a very similar thing with the Fibonacci numbers.
27:44
So Ryan Chongli Grant Yes. And. Or is our team's hand-drawn?
27:51
Oh yeah, I didn't raise my hand. Yeah, but actually Hans Shonga, they seem to linger and some random.
28:01
It's a well engineered piece of software. OK, take it out to numbers, yeah.
28:08
So this is RFIDs, and let's let's let's try to massage this a little bit.
28:14
So let's do tail of this. See what? See what they gave us. So the team on the signature numbers is obviously.
28:20
We just to the first element. Now, interestingly, when we lined these up next to each other, if you look at the the numbers, they match up.
28:29
And if you add them, so let's call that, you know, zip with plus the two lists.
28:41
That's two, three, five eight, 13, 21, etc. So if you take the Fibonacci numbers and then.
28:49
Add that with the Fibonacci numbers and the tail of the Fibonacci numbers.
29:04
What you get is two three, five eight, 13, 21, and obviously now that's not quite what we want.
29:09
We need to fix that by pretending one Collins one at the beginning of this list.
29:16
And so what this does is, you know, produces the first two elements and then magically looking at, you know, knowing what the list should be.
29:21
And then the tail of that list adding the elements together magically, if you value this, that produces the list of Fibonacci entries.
29:34
And you know, I can run this infinitely and it all just keep going and give me a larger and larger Fibonacci numbers.
29:47
And I claim that this is the best way to to implement the Fibonacci numbers.
29:56
It's it's single. It's a one liner, it's deeply magical and, you know, perfect for the for the Halloween season.
30:00
The if you if you think about what's going on here, it's actually in terms of complexity.
30:10
This is this is linear as well because it's very similar to the tail recursive version because it's actually just taking the last two elements.
30:18
And every time you take a step into the next element of the list,
30:32
it just uses it with plus function to or rather using the plus function to to zip together the lists.
30:36
Previous element with the next element. And. So this actually generates a list as opposed to the tail function.
30:44
So the space complexity of this is also linear, whereas pigtail was just constants based.
30:50
But I wouldn't say that this is wasted space because actually we get an infinite list of of numbers.
30:56
So it's all the values are given back to us. So.
31:03
Phelps, let's look at the thousand elements.
31:10
So it's very quick. So again, in there, are there any questions about this?
31:15
I have a question.
31:23
So if we want to fund the UN's term of PHEVs, we still have a score complexity because we must run out from the first to the Earth turn.
31:24
Well, so. Computing the individual elements so that the only thing is like, look at this list,
31:41
look up here is the veneer, of course, in the in the list, but the computation itself does not do lookups.
31:49
It. Operationally, what happens is the it just goes off and, you know,
31:58
it does one one and then the next element is it looks back at what's been already produced, say always looks at the.
32:06
It always it's always stays at the end of the list, so to speak, so it doesn't require a linear time to look up.
32:18
It's it's just a list and it's being produced is the way the recursion works out.
32:24
Thanks. Thanks. This definition is a and it doesn't, you know, it doesn't mean it doesn't regard walking to all this again.
32:35
It's so unique in the challenge relevant.
32:44
We only like Take one. Yeah, OK.
32:49
Yeah, that's right. So if you actually compare, you know, because I did fit, was it 10000 finishes really quickly?
32:55
It's prime 10000. Sorry, Prime ten thousand.
33:03
It's way slower, so it's obviously not a formal proof that this is linear,
33:10
but it kind of gives you an idea that it doesn't do as much work as the table look operation.
33:15
And in fact, it's comparable to the big tail function.
33:22
And as we, you know. Which cancel out.
33:26
So it's 10000 still very fast.
33:32
It takes the in fact, it takes longer to print the number than to compute it and similarly, you know, fishtail.
33:38
To reload. Same thing, so every 15 is a little bit faster because it just keeps the number in a linear way and then boom, you're done.
33:49
Whereas with feds, you have to build up the list and then do a lookup.
34:01
So it's, you know, maybe the constant factor is a little bit worse for sale. But, you know,
34:05
other than it's just one one thought is that if you think about why it's work and the
34:12
definition of it with will apply the plus function to the head of the two lists each time,
34:16
that's what's the time for each of each. Plus is a constant time order one.
34:22
Yeah. For that reason, the definition is it to see that, but they've done that.
34:26
So. Yeah, that's right. Yeah. So it's always we are always always accessing the heads of the lists and then
34:30
we're making references back to the sort of current heads as we build up.
34:38
And yeah, so this is this is linear linear function.
34:44
OK, so there are no more questions about this then. So you have one final.
34:50
We have some time. Yeah. Hush hush up a question.
34:54
Actually Russian. The difference between doing it with zip with and with doing it with Scandal.
34:58
Because if you cannot withstand left, you can do it with a plus as well.
35:08
If he chose, I pasted in one of my like a slip of code, I just made really quickly.
35:16
Let's see. See?
35:23
OK, this looks like it'll work to.
35:33
It's fine. Yeah.
35:41
I mean, you know, you need to. It's not quite right, is it?
35:46
Maybe. Well, it's not quite right.
35:56
I'm sure there's a way to fix it, but. Yeah, because you need to.
36:01
You need to access to the last two elements and.
36:10
This version only builds on the last element.
36:17
Actually, maybe me, but I was wrong. Sorry. OK.
36:22
Yeah, so. So it's prime, prime is one concept prime.
36:27
OK. Yeah, that's right, sir. Yeah.
36:35
No, you can do it like that, as well as like there's no one true way of implementing this function.
36:39
So that's really cool. I quite like that. So thanks for thanks for sharing.
36:48
Yeah, so we have a few minutes. I wanted to talk about one last kind of fun thing that we can we can do with with these recursive functions.
36:56
So I wanted to compare. Seb and I wanted to comment it out on Prime.
37:10
I just kind of draw. Attention to this similarity in the structure of these two functions is basically.
37:20
They're both. They both have the same shape where five zero equals one prime equals zero equals one big one equals one prime equals one.
37:30
And then in sort of the generic case, it does a recursive call back to itself.
37:41
And Fed Prime does not see a recursive call,
37:51
but it does also call a function with the argument and minus one and then adds it to the
37:54
same function minus two with with fairly also the Fed and minus one plus minus two.
38:00
Now, to make it a little bit more apparent that that's what's going on here.
38:08
You know, this one doesn't look like a function call with the argument and minus one, but actually if you bracket things like this.
38:14
I'll show you this in the in the actual version, so you can see I'm not cheating.
38:25
If you bracket things like this and it's going to complain that it's redundant, the bracketing is redundant.
38:31
But but it works so that Prime 10 still so gives us the result if you haven't seen this before.
38:38
You know, in high school f a b, so the function f apply to two arguments and B is the same as f a applied to be.
38:45
Some took so function application associates to the left to the left in this way.
38:54
And so that's why we're allowed to put the brackets around here.
39:00
And if you do that, then it becomes apparent that the you know,
39:05
this is also calling some function within with argument minus one and then added to some function argument I might do.
39:11
And why am I saying this whenever there's a, you know, in programming in general, whenever you see some similarity between two functions,
39:19
you can do some refactoring to factor out the common bit and then instantiate that common into the two specific versions that you have.
39:27
So we sometimes call this abstraction.
39:39
So way to do this abstraction is to say let's make a version of FIB, which is not very good with coming up names.
39:43
So this is going to be a fit h for for high order. Now what's it going to do is take a function as an argument.
39:52
And simply call that function. You know, so for the ages now, as I alluded to, it is a higher order function because it itself.
40:04
It's a function that takes another function as an argument.
40:15
And the reason I called it Kay is, you know, in the literature, this function, it's sometimes called a continuation and the sometimes for some reason.
40:18
So, so case the continuation function.
40:26
And the idea here is that fib age is only responsible for providing the shape of the computation,
40:30
but not the actual, you know, doesn't tell us what to do with the.
40:36
In a recursive steps, so to speak, so give it signature to Sibiraj.
40:42
So now obviously, it takes a function as an argument and we can get back to the original fib.
40:50
By calling FIB Age with. Fifth and sixth or edge and right?
40:57
So or. Now, of course, fair, there's going to be an.
41:09
We're going to do the research itself after the first step, so that's not quite what we want yet, but I'll come back to that to that in a second.
41:22
Fed table. Table Prime Core that this one is, you know, this is the second version.
41:32
Mideast conflict, prime, prime, that doesn't exist yet.
41:48
So it's factoring out the second version, you know, what we do is we plug into the continuation.
41:52
This will look up into the table. So that thing goes in there and again, the same thing goes in there, too.
41:57
So fed prime prime of ten, you know, this does exactly the same thing.
42:08
But now we don't have to. You know, duplicate the logic here.
42:14
We don't have to spell out the base case and then the logic for the recursive step,
42:21
all we need to do is say, OK, what is this higher order version where people bang, bang?
42:26
So, so that works out well. Now I said this chip orange is not quite satisfactory yet because it.
42:33
And. OK.
42:44
Yes, actually, I wanted to yeah, I wanted to plug in the function itself, so.
42:50
Because when you just plug into big. Then, you know, it'll actually just expand to.
42:57
You know. Bed Fed said it will just expand to this, which then, you know,
43:07
the subsequent Fed functions, or it just goes back to the original recursive definition.
43:15
But if you plug in this thing itself a ridge, then what this is doing is recursively,
43:20
you know, the higher end function will recursively put itself back into that position.
43:33
So the K will be Fable Ridge, which, if you expand, is fit age of origin, right?
43:39
So and so the recursive step is going to be save age of minus one.
43:46
But this is again. Age range and then.
43:56
The original and it kind of keeps going infinitely like that, right,
44:07
because the the function is recursive in this way and you're plugging itself back.
44:13
So. It's a sort of a final point.
44:20
Maybe we've got a couple of minutes. I mean, let's let's look at whether we can factor out this recursion once more.
44:24
And so what I'm what I mean by that is I want to I want to write for Burridge in a way, but on the right hand side, it doesn't mention itself.
44:36
So it will no longer be explicitly recursive, right?
44:46
Because because it mentions itself, so it'll keep expanding in that way. Now again, thanks to laziness in high school,
44:50
what we can do is define a higher order function that given the original, give it or given any other function indeed.
44:58
Well. It infinitely expand the the function application, so we call this a fixed point, and usually so the fixed point of F is.
45:08
If applied to fix up. In general, when X is a fixed point, X is a fixed point of F when X equals fixed, right, so this is in.
45:24
You know, Max, as you know, and you do function, obviously,
45:38
as you say at this point in this function is that is the point where you keep applying the function, you get the same thing.
45:42
So this is the literal definition.
45:48
Fixed point of F is equal to f applied to the fixed point.
45:52
Right? Sorry. It's when it's because it affects.
45:57
Sorry, this is the right thing, yeah, so that's the fixed point now.
46:09
What we can do is plug in. The fixed point of age.
46:16
And so while this is going to do exactly here, what I wrote out here is take 50 Sibiraj,
46:29
the pirated version of that publicity function and keep applying it to itself infinitely many times.
46:36
And if I do that, then you know, we go back to the original slow version of fame.
46:46
But now this version is no longer explicitly a recursive.
46:53
We just use this one magical recursion combinator to say, you know, basically it says I want to.
46:57
I want a version of this page where the function we plug in to the Constitution is age itself,
47:07
so it's fixed by calling nature gives us that self application and the type of that is this, by the way, and I think that's the last thing.
47:14
So if you have any questions, I'm happy to say that now. You see some hands up, Dhruv, I think you were first.
47:23
I just wanted to know what what benefit do we get by not explicitly recursive.
47:36
Oh, so what was the benefit of doing this? Yes. Absolutely nothing.
47:42
Or. Because it explicitly rebels or not, so we are just shifting the problem.
47:47
Yeah, I mean, it's, you know, it's one of those things that. There are if you go into Computer Village theory,
47:54
you'll find a bunch of these special purpose programming languages that don't have recursive functions,
48:01
or even if you're if you yourself want to implement your own programming language,
48:07
then maybe you'll find that implementing recursive functions is difficult in general.
48:10
So you'll just say, OK, I'm going to not allow recursive functions and but our higher order functions and provide this fixed point combinator.
48:15
And in fact, it turns out that every recursive function can be factored out into using this
48:24
fixed point combinator and higher order version of the original function.
48:31
So it's kind of interesting until I tried to that next year, I'll be looking at lambda calculus and lambda calculus.
48:37
You'll discover that that fixed point function can be defined without recursion at all.
48:43
So in fact, you have a programming language which essentially doesn't have any recursion,
48:52
but it's still able to construct the circular references by black magic.
48:55
And that is an entirely Turing complete programming language, as you'll discover next year.
49:01
I think next person was Jake. Yeah.
49:07
What is fix, not Luke, since it's just calling itself, and there's no base case for it to?
49:12
Yeah, yeah. So very good question. Why this fix?
49:17
Not Luke? Well, doesn't Luke, because Pascal's lazy.
49:21
So um, of course, it depends on what you plug into.
49:25
So when you say fix of H?
49:30
It's fit age that's responsible for for providing the base case, which it does, because once you reach the age of, you know, zero, it'll stop.
49:33
So actually, even though in the recursive case of people ridge, you'll have a point where you know you're applying fix age to and minus one.
49:44
And this is going to be itself an infinite chain of ages.
49:55
But because high school's lazy, it's only ever going to force that as many times as as it needs.
50:02
And then once that reaches the base case, it'll give up.
50:10
So you couldn't do this fix in a in a in a strict language,
50:14
you would need another version that does something you know, like the court is the closure passing style something.
50:19
So this is called the Y Combinator.
50:29
Specs, function and in truth, languages, you can't have the Y Combinator, you can have something coders, said Combinator.
50:34
And that works, you know, you can look it up. It works for four straight languages as well, and it makes a little bit more apparent.
50:41
Why doesn't move in infinitely in that case? But these languages, like Haskell, you can get away with just defining like this.
50:49
Now, of course, if you provide a function that does not have a base case, so, you know, fix it, Penn.
50:56
You know, that will never terminate because it just keeps flying, I'd infinitely.
51:05
And there's no point where it says, OK, I'm done now. Whereas which Page Age actually does have a base case now if I remove these base cases?
51:12
It's never going to terminate. Right. So you have to be careful.
51:23
And this is the point where. When GCI sesshin died was probably a.
51:29
OK. Yeah, so, yeah, that's a great question, and it's the reason is, is that OK, I think we've got time for make you Jing'an.
51:41
Is that is that an old hand? It seems hundreds of and actually so when talking about the time complexity, actually,
51:52
we have some like faster way to implement Fibonacci by using something like Matrix.
52:03
But. Is there any like easy way to like this matrix, you, Haskell?
52:11
Well, actually, it's a, you know, it's a very quick way of imposing Fibonacci by so has a close formula which you can evaluate in just in time.
52:20
So there's always a faster way. I'm not sure about what you mean.
52:32
Is your question whether Haskell has a matrix data structure that you can use, in which case the answer is yes.
52:38
You know, any day structure is possible to implement in Haskell, and you can always even mutable things, but that's kind of a more advanced topic.
52:43
But yeah, I suppose a general purpose programming language, you can implement everything or anything.
52:53
And also. Very efficient ways.
52:59
So the answer to any question of can you do this in high school is always, yes, you can do everything.
53:02
Yes, a high school has a race we don't cover in the lectures, maybe I should.
53:11
But of course, you talked about the array when you construct it, so it'll be limited to a certain number of.
53:16
Well, actually, we cover the use of a reason the algorithm design and analysis course in the second year.
53:21
That's true. That's all coming along. Yeah. Thank you for reminding me.
53:27
I think we got time, but we we should stop. So if people need to go, you should go. But Dhruv, you have is that an old hand-drawn U.N.?
53:33
I think it seems in my team, right? I think people stop, so let's thank general thank you very much, John.
53:41
That was fantastic. Well done. And we'll do the same thing again next week.
53:48
We'll have another topic. We'll work out a Carmen, what's on the schedule, but one of us will give it and please come along.
53:51
Thank you. Nice to see it. Thanks, Schmidt. Thank you.
53:58
It's.
54:03