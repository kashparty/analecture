ID: 4e81830c-c77a-477c-86ee-ae160107280c
Title: 1.2 Representations
Category: COMP40008 - Graphs and Algorithms (Spring 2021-2022)
Lecturer: Iain Phillips
Date: 15/01/2021
So now I want to talk about how to represent graphs on computers,
0:01
and I'm going to look at two popular representations, namely adjacency matrices and adjacency lists.
0:07
So let's start with adjacency matrices. Let's take our example graph with the seven nodes.
0:15
So we're going to create an adjacency matrix, which will be a seven by seven.
0:23
So down the road, we got ABC, T, e, f, g and across the columns and the IJA entry will be how many nodes connect I to J.
0:27
So references A is connected to G twice. So that gives us this entry to in the H-E entry place.
0:39
Now notice that this graph is going to be symmetric because the arcs are undirected.
0:50
So if we have an A G, then we're going to have the same number going from G to a.
0:54
That would not be true when we come to direct graphs later on.
1:00
Notice that if we order the nodes differently, the matrix gets rearranged.
1:06
But there's nothing particularly special here.
1:10
I mean, I've gone ABC d e f g just conventionally following the alphabet, but any other order will be okay.
1:12
Providing we're consistent in the order of when the rows and the columns.
1:20
What happened was on the diagonal entries. That's for the loops. I want to count each loop twice.
1:26
So I'm going to book for the C C I'm going to put entry, I'm going to put in two there.
1:32
And that will be consistent with our definition of degree. So that every hour contributes twice to the adjacency matrix.
1:39
So if you think about it, if you take any row then and you add up the entries there, that will just give you the degree of that node.
1:45
Similarly, if you add up the columns, given that the matrix is symmetric.
1:53
So that's adjacency matrices here.
1:59
How about adjacency lists? That's our other favourite representation.
2:03
So an adjacency list is an array of link lists. So here is we have an array for the nodes.
2:09
In this case, I've numbered them rather than giving them letters here just to be a bit more standard.
2:17
So here is the array on the left with the one up to seven.
2:25
And then we've got for each of these.
2:31
That is the starting point of a linked list to a linked list is a list where you have a start point which and then you follow the links.
2:33
These arrows from node node getting the values in the list.
2:41
So and this list is going to be all the no's which are adjacent to the corresponding entry in the array.
2:46
So for instance, one is adjacent to seven twice. So we'll put it in to the link list twice.
2:54
And four is adjacent to three and five. Three, of course.
3:02
What to do about the loop? Well, I've just put it in the wants here.
3:06
So what we can see is that multiple arcs give us multiple entries like the seven seven here, but the loop just gets entered once.
3:13
But apart from that, each arc gets entered twice. So between six and two, we've got an arc that's entered wants that the two and wants for the six.
3:22
OK. So that's the adjacency list representation. Let's compare the two representations.
3:33
The adjacency matrix gives us efficient access to any arc in the graph.
3:41
We just look up the IJA entry and we see, is there an arc there or isn't there?
3:46
But certain algorithms rely on just going round nodes in the graph and looking at all arcs incident.
3:53
On that note. So we'll see, for instance, depth, first breadth, first search algorithms, for instance.
4:00
And for these algorithms, it can be faster and sort of going with the grain of the algorithm.
4:09
Better to use adjacency lists. And that will be particularly the case if the Matrix and the adjacency matrix has a lot of zeros in it.
4:16
In other words, the graph is a sparse graph. So what do I mean by sparse?
4:24
Well, let's suppose that there are N nodes and M arcs.
4:29
Then the adjacency matrix just has a fixed size, which is N squared.
4:34
It doesn't matter how many or how few arcs there are. It may and may be M is very small.
4:37
And so we've got loads of zeros in there. Which is a bit inefficient. On the other hand, the adjacency matrix has size.
4:44
And because we've got to have the array plus two M well less equal to two M because each arc occurs twice basically.
4:51
So those are rough figures for the size I should really put Bigos in,
5:02
which I'm coming onto in a moment to be a bit more accurate in my statement here.
5:06
And let's say that a graph is sparse. If M is much smaller than N squared.
5:12
Well, in that case you can see that the storage for the adjacency list is going to be much less than the than the one for the adjacency matrix.
5:18
Right. So let's talk a bit about Bigo notation that I just mentioned.
5:32
Let's take as an example, multiplying and by and matrices to get the square matrices.
5:36
So here's an example for three by three matrices. And how do we compute the product?
5:41
Well, we basically take a row from the first matrix and a column from the second matrix and multiply together the entries and add the results.
5:48
So you can see that each entry on the right here takes us.
5:57
And multiplications and N minus one additions. In this case, three multiplications and two additions for an equals three.
6:00
So we add up the number of multiplication. Of course, they're N squared entries here.
6:09
So that's n cute multiplications in all or the additions would be cubed minus N squared.
6:13
Or we might just add the two together and get two N cubed minus N squared arithmetical operations.
6:21
Well watch. Which of these is the best measure of how much effort we're taking here?
6:27
Well, probably the multiplications are more effort than the addition. So we might decide that.
6:33
And due to multiplications is a reasonable measure.
6:38
On the other hand, we might decide that the arithmetical operations, that would be a good measure, too.
6:41
So we don't know quite which one to choose. But you can see that in any case, whichever of these I choose.
6:47
Then basically the N cubed is the term. That's the limiting term here.
6:54
The one that's growing fast is the one that we would like to improve on if possible.
6:58
So we can get away from having to make this decision. And after all, the N cubed is not quite right.
7:04
As it stands, because, you know, the actual speed here will depend on how fast each individual multiplication is.
7:10
And that means we have to know something about the architecture that are using and things like that.
7:17
So to get away from being too precise, then we can talk in terms of big O notation.
7:22
So Bigo enter the K will mean bounded by enter the K, ignoring constant factors.
7:29
So the first case is just bago and cubed multiplications.
7:36
Nearly the second case still Bego and cubed.
7:41
Even though it's a bit smaller. And the third case is Bago and cubed because we can ignore the constant factor too.
7:45
And that sort of get smuggled into the Bago notation. So we have solved our problem here.
7:52
We don't have to be too precise. Not exactly what we're measuring here.
7:57
The answer is that multiplication of N by and matrices is Bigo of eight and cubed by this kind of measure.
8:02
So the idea and the big O notation is that we can ignore the constant factors and the less important terms.
8:10
So, for instance, if something takes constant time, let's say three hundred seventy one,
8:18
we can just call that bigo of Mom because we're ignoring the three seven one, which is the constant factor.
8:24
Linear time. Let's say twenty five N plus four. We can just write that just Bego of N.
8:30
So we get rid of the 25 constant factor and we ignore the four is being less important.
8:36
They go off and squared that we can call that quadratic. So here is a quadratic expression.
8:43
And again, you can see that the N squared is the important term. We can neglect the smaller terms because as N gets large, it's the end squared.
8:49
That dominates. And we could ignore the constant factor, too.
8:58
So we will be using big AI notation when calculating the space and time usage of data structures and algorithms.
9:03
And the advantage of this is that the abstracts away from implementation dependent specifics,
9:10
like exactly how fast our particular computer is when it comes to doing matrix multiplication.
9:15
And we concentrate on the factor which determines the growth, which is the the the term with the.
9:22
In this case, in the case of polynomials, the time with the highest order here, the N squared.
9:28
And of course, I'd be a little bit vague here, but this is good enough for us for the moment.
9:34
And I'll give precise definitions of the big O notation later in the course.
9:41